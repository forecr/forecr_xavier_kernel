// SPDX-License-Identifier: GPL-2.0-only OR MIT
// SPDX-FileCopyrightText: Copyright (c) 2023-2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.

#include <nvgpu/dma.h>
#include <nvgpu/log.h>
#include <nvgpu/bug.h>
#include <nvgpu/debug.h>
#include <nvgpu/enabled.h>
#include <nvgpu/fuse.h>
#include <nvgpu/debugger.h>
#include <nvgpu/error_notifier.h>
#include <nvgpu/io.h>
#include <nvgpu/utils.h>
#include <nvgpu/bitops.h>
#include <nvgpu/gk20a.h>
#include <nvgpu/regops.h>
#include <nvgpu/gr/ctx.h>
#include <nvgpu/gr/config.h>
#include <nvgpu/gr/gr.h>
#include <nvgpu/gr/gr_instances.h>
#include <nvgpu/gr/warpstate.h>
#include <nvgpu/channel.h>
#include <nvgpu/engines.h>
#include <nvgpu/engine_status.h>
#include <nvgpu/fbp.h>
#include <nvgpu/nvgpu_err.h>
#include <nvgpu/netlist.h>
#include <nvgpu/grmgr.h>
#include <nvgpu/gr/obj_ctx.h>
#include <nvgpu/gr/gr_utils.h>

#include "gr_gb10b.h"
#include "hal/gr/gr/gr_gk20a.h"
#include "hal/gr/gr/gr_gv11b.h"
#include "hal/gr/gr/gr_ga10b.h"
#include "hal/gr/gr/gr_pri_gk20a.h"
#include "hal/gr/ctxsw_prog/ctxsw_prog_gb10b.h"
#include "common/gr/gr_priv.h"
#include "common/gr/obj_ctx_priv.h"
#include "hal/lrc/lrc_gb10b.h"

#include <nvgpu/hw/gb10b/hw_gr_gb10b.h>
#include <nvgpu/hw/gb10b/hw_proj_gb10b.h>
#include <nvgpu/hw/gb10b/hw_ctxsw_prog_gb10b.h>

#define ILLEGAL_ID	~U32(0U)

u32 gb10b_gr_gpc0_ppc0_cbm_alpha_cb_size(void)
{
	return gr_gpc0_ppc0_cbm_alpha_cb_size_r();
}

/**
 * These are NV_PGRAPH_GPC0_TPCS_CAU_* registers.
 * NV_PGRAPH_PRI_GPC0_TPCS_CAU_CONTROL(0),
 * NV_PGRAPH_PRI_GPC0_TPCS_CAU_PERFMONID(0),
 * NV_PGRAPH_PRI_GPC0_TPCS_CAU_CYCLECNT_LSB(0),
 * NV_PGRAPH_PRI_GPC0_TPCS_CAU_CYCLECNT_MSB(0),
 * NV_PGRAPH_PRI_GPC0_TPCS_CAU_TOTAL_TRIG_RCV(0),
 * NV_PGRAPH_PRI_GPC0_TPCS_CAU_STATE(0),
 * NV_PGRAPH_PRI_GPC0_TPCS_CAU_ENGINESTATUS(0),
 * NV_PGRAPH_PRI_GPC0_TPCS_CAU_CONTROLB(0),
 */
static const u32 hwpm_cau_init_data[] =
{
    /* This list is autogenerated. Do not edit. */
	0x00504180,
	0x00000000,
	0x00504188,
	0x00000000,
	0x0050418c,
	0x00000000,
	0x00504190,
	0x00000000,
	0x00504194,
	0x00000000,
	0x00504198,
	0x00000000,
	0x0050419c,
	0x00000000,
	0x005041a4,
	0x00000001,
};

const u32 *gb10b_gr_get_hwpm_cau_init_data(u32 *count)
{
	*count = sizeof(hwpm_cau_init_data) / sizeof(hwpm_cau_init_data[0]);
	return hwpm_cau_init_data;
}

int gb10b_gr_init_cau(struct gk20a *g, u32 gr_instance_id)
{
	const u32 *data;
	u32 cau_stride;
	u32 num_cau;
	u32 count;
	u32 i, j, k;
	u32 start_gpc = nvgpu_grmgr_get_gr_gpc_logical_id(g, gr_instance_id, 0x0U);
	u32 num_gpc = nvgpu_gr_config_get_gpc_count(nvgpu_gr_get_config_ptr(g));
	u32 max_gpc_idx = start_gpc + num_gpc;
	u32 result = 0U;
	u32 reg = 0U;

	num_cau = gr_gpcs_tpcs_cau_control__size_1_v();
	cau_stride = g->ops.regops.get_cau_register_stride();

	data = g->ops.gr.get_hwpm_cau_init_data(&count);

	for (k = start_gpc; k < max_gpc_idx; k++) {
		for (i = 0U; i < num_cau; i++) {
			for (j = 0U; j < count; j += 2U) {
				if (nvgpu_safe_mult_u32_return(i, cau_stride, &result) == false) {
					nvgpu_err(g, "Buffer overflow");
					return -EOVERFLOW;
				}
				if (nvgpu_safe_mult_u32_return(k, proj_gpc_stride_v(), &reg) ==
											false) {
					nvgpu_err(g, "Buffer overflow");
					return -EOVERFLOW;
				}
				if (nvgpu_safe_add_u32_return(data[j], reg, &reg) == false) {
					nvgpu_err(g, "Buffer overflow");
					return -EOVERFLOW;
				}
				if (nvgpu_safe_add_u32_return(reg, result, &reg) == false) {
					nvgpu_err(g, "Buffer overflow");
					return -EOVERFLOW;
				}
				nvgpu_writel(g, reg, data[j + 1U]);
			}
		}
	}

	if (g->ops.priv_ring.read_pri_fence != NULL) {
		g->ops.priv_ring.read_pri_fence(g);
	}
	return 0;
}

int gb10b_gr_disable_cau(struct gk20a *g, u32 gr_instance_id)
{
	u32 i, j, k, reg, num_tpc;
	u32 gpc_stride = proj_gpc_stride_v();
	u32 tpc_stride = proj_tpc_in_gpc_stride_v();
	u32 start_gpc = nvgpu_grmgr_get_gr_gpc_logical_id(g, gr_instance_id, 0x0U);
	u32 num_gpc = nvgpu_gr_config_get_gpc_count(nvgpu_gr_get_config_ptr(g));
	u32 max_gpc_idx = start_gpc + num_gpc;
	u32 result = 0U;

	/* GPC(n), TPCS control registers are programmed to 0x0 */
	for (i = 0U; i < gr_gpcs_tpcs_cau_control__size_1_v(); ++i) {
		for (j = start_gpc; j < max_gpc_idx; j++) {
			/*
			 * TODO: NVGPU-10557 Replace the below hal usage with
			 * nvgpu_gr_config_get_gpc_tpc_count once the sw struct is correctly
			 * updated with tpc count per gpc.
			 */
			num_tpc = g->ops.gr.config.get_tpc_count_in_gpc(g,
							nvgpu_gr_get_config_ptr(g), j);
			for (k = 0; k < num_tpc; k++) {
				if (nvgpu_safe_mult_u32_return(gpc_stride, j, &result) == false) {
					nvgpu_err(g, "Buffer overflow");
					return -EOVERFLOW;
				}
				if (nvgpu_safe_mult_u32_return(tpc_stride, k, &reg) == false) {
					nvgpu_err(g, "Buffer overflow");
					return -EOVERFLOW;
				}
				if (nvgpu_safe_add_u32_return(reg, result, &reg) == false) {
					nvgpu_err(g, "Buffer overflow");
					return -EOVERFLOW;
				}
				if (nvgpu_safe_add_u32_return(gr_gpc0_tpc0_cau_control_r(i),
									reg, &reg) == false) {
					nvgpu_err(g, "Buffer overflow");
					return -EOVERFLOW;
				}
				nvgpu_writel(g, reg, 0U);
			}
		}
	}

	if (g->ops.priv_ring.read_pri_fence != NULL) {
		g->ops.priv_ring.read_pri_fence(g);
	}

	return 0;
}

u32 gb10b_get_sm_dbgr_ctrl_base(void)
{
	return gr_gpc0_tpc0_sm0_dbgr_control0_r();
}

u32 gb10b_get_sm_dbgr_status_base(void)
{
	return gr_gpc0_tpc0_sm0_dbgr_status0_r();
}

u32 gb10b_get_sm_hww_global_esr_base(void)
{
	return gr_gpc0_tpc0_sm0_hww_global_esr_r();
}

u32 gb10b_get_sm_hww_warp_esr_base(void)
{
	return gr_gpc0_tpc0_sm0_hww_warp_esr_r();
}

u32 gb10b_get_gpcs_pri_mmu_debug_ctrl_reg(void)
{
	return gr_gpcs_pri_mmu_debug_ctrl_r();
}

u32 gb10b_get_gpcs_tpcs_tex_in_dbg_reg(void)
{
	return gr_gpcs_tpcs_tex_in_dbg_r();
}

u32 gb10b_get_gpcs_tpcs_sm_l1tag_ctrl_reg(void)
{
	return gr_gpcs_tpcs_sm_l1tag_ctrl_r();
}

static void gb10b_gr_dump_gr_per_sm_regs(struct gk20a *g,
			struct nvgpu_debug_context *o,
			u32 gpc, u32 tpc, u32 sm, u32 offset)
{
	gk20a_debug_output(o,
		"NV_PGRAPH_PRI_GPC%d_TPC%d_SM%d_HWW_WARP_ESR: 0x%x\n",
		gpc, tpc, sm, nvgpu_readl(g,
		nvgpu_safe_add_u32(gr_gpc0_tpc0_sm0_hww_warp_esr_r(),
				   offset)));

	gk20a_debug_output(o,
		"NV_PGRAPH_PRI_GPC%d_TPC%d_SM%d_HWW_WARP_ESR_REPORT_MASK: 0x%x\n",
		gpc, tpc, sm, nvgpu_readl(g,
		nvgpu_safe_add_u32(gr_gpc0_tpc0_sm0_hww_warp_esr_report_mask_r(),
				   offset)));

	gk20a_debug_output(o,
		"NV_PGRAPH_PRI_GPC%d_TPC%d_SM%d_HWW_GLOBAL_ESR: 0x%x\n",
		gpc, tpc, sm, nvgpu_readl(g,
		nvgpu_safe_add_u32(gr_gpc0_tpc0_sm0_hww_global_esr_r(),
				   offset)));

	gk20a_debug_output(o,
		"NV_PGRAPH_PRI_GPC%d_TPC%d_SM%d_HWW_GLOBAL_ESR_REPORT_MASK: 0x%x\n",
		gpc, tpc, sm, nvgpu_readl(g,
		nvgpu_safe_add_u32(gr_gpc0_tpc0_sm0_hww_global_esr_report_mask_r(),
				   offset)));

	gk20a_debug_output(o,
		"NV_PGRAPH_PRI_GPC%d_TPC%d_SM%d_DBGR_CONTROL0: 0x%x\n",
		gpc, tpc, sm, nvgpu_readl(g,
		nvgpu_safe_add_u32(gr_gpc0_tpc0_sm0_dbgr_control0_r(),
				   offset)));

	gk20a_debug_output(o,
		"NV_PGRAPH_PRI_GPC%d_TPC%d_SM%d_DBGR_STATUS0: 0x%x\n",
		gpc, tpc, sm, nvgpu_readl(g,
		nvgpu_safe_add_u32(gr_gpc0_tpc0_sm0_dbgr_status0_r(),
				   offset)));
}

static void gb10b_gr_dump_gr_sm_regs(struct gk20a *g,
			   struct nvgpu_debug_context *o)
{
	u32 gpc, tpc, sm, sm_per_tpc;
	u32 gpc_offset, tpc_offset, offset;
	struct nvgpu_gr *gr = nvgpu_gr_get_cur_instance_ptr(g);

	gk20a_debug_output(o,
		"NV_PGRAPH_PRI_GPCS_TPCS_SMS_HWW_GLOBAL_ESR_REPORT_MASK: 0x%x\n",
		nvgpu_readl(g,
		gr_gpcs_tpcs_sms_hww_global_esr_report_mask_r()));
	gk20a_debug_output(o,
		"NV_PGRAPH_PRI_GPCS_TPCS_SMS_HWW_WARP_ESR_REPORT_MASK: 0x%x\n",
		nvgpu_readl(g, gr_gpcs_tpcs_sms_hww_warp_esr_report_mask_r()));
	gk20a_debug_output(o,
		"NV_PGRAPH_PRI_GPCS_TPCS_SMS_HWW_GLOBAL_ESR: 0x%x\n",
		nvgpu_readl(g, gr_gpcs_tpcs_sms_hww_global_esr_r()));
	gk20a_debug_output(o,
		"NV_PGRAPH_PRI_GPCS_TPCS_SMS_DBGR_CONTROL0: 0x%x\n",
		nvgpu_readl(g, gr_gpcs_tpcs_sms_dbgr_control0_r()));
	gk20a_debug_output(o,
		"NV_PGRAPH_PRI_GPCS_TPCS_SMS_DBGR_STATUS0: 0x%x\n",
		nvgpu_readl(g, gr_gpcs_tpcs_sms_dbgr_status0_r()));
	gk20a_debug_output(o,
		"NV_PGRAPH_PRI_GPCS_TPCS_SMS_DBGR_BPT_PAUSE_MASK_0: 0x%x\n",
		nvgpu_readl(g, gr_gpcs_tpcs_sms_dbgr_bpt_pause_mask_0_r()));
	gk20a_debug_output(o,
		"NV_PGRAPH_PRI_GPCS_TPCS_SMS_DBGR_BPT_PAUSE_MASK_1: 0x%x\n",
		nvgpu_readl(g, gr_gpcs_tpcs_sms_dbgr_bpt_pause_mask_1_r()));

	sm_per_tpc = nvgpu_get_litter_value(g, GPU_LIT_NUM_SM_PER_TPC);
	for (gpc = 0U;
	     gpc < nvgpu_gr_config_get_gpc_count(gr->config); gpc++) {
		gpc_offset = nvgpu_gr_gpc_offset(g, gpc);

		for (tpc = 0U;
		     tpc < nvgpu_gr_config_get_gpc_tpc_count(gr->config, gpc);
		     tpc++) {
			tpc_offset = nvgpu_gr_tpc_offset(g, tpc);

			for (sm = 0U; sm < sm_per_tpc; sm++) {
				offset = nvgpu_safe_add_u32(
						nvgpu_safe_add_u32(gpc_offset,
								   tpc_offset),
						nvgpu_gr_sm_offset(g, sm));

				gb10b_gr_dump_gr_per_sm_regs(g, o,
					gpc, tpc, sm, offset);
			}
		}
	}
}

static void gb10b_gr_dump_tpc_activity_regs(struct gk20a *g,
					    struct nvgpu_debug_context *o)
{
	struct nvgpu_gr *gr = nvgpu_gr_get_cur_instance_ptr(g);
	u32 gpc_index = 0U;
	u32 tpc_count = 0U, tpc_stride = 0U;
	u32 reg_index = 0U, offset = 0U;
	u32 i = 0U;

	if (nvgpu_gr_config_get_base_count_gpc_tpc(gr->config) == NULL) {
		return;
	}

	tpc_count = nvgpu_gr_config_get_gpc_tpc_count(gr->config, gpc_index);
	tpc_stride = nvgpu_get_litter_value(g, GPU_LIT_TPC_IN_GPC_STRIDE);

	for (i = 0U; i < tpc_count; i++) {
		offset = nvgpu_safe_mult_u32(tpc_stride, i);
		reg_index = nvgpu_safe_add_u32(offset,
				gr_pri_gpc0_tpc0_tpccs_tpc_activity_0_r());

		gk20a_debug_output(o,
			"NV_PGRAPH_PRI_GPC0_TPC%d_TPCCS_TPC_ACTIVITY0: 0x%x\n",
			i, nvgpu_readl(g, reg_index));
	}
}

int gb10b_gr_dump_fecs_gr_status_regs(struct gk20a *g,
				 struct nvgpu_debug_context *o)
{
	gk20a_debug_output(o, "NV_PGRAPH_STATUS: 0x%x\n",
		nvgpu_readl(g, gr_status_r()));
	gk20a_debug_output(o, "NV_PGRAPH_STATUS1: 0x%x\n",
		nvgpu_readl(g, gr_status_1_r()));
	gk20a_debug_output(o, "NV_PGRAPH_INTR: 0x%x\n",
		nvgpu_readl(g, gr_intr_r()));
	gk20a_debug_output(o, "NV_PGRAPH_EXCEPTION  : 0x%x\n",
		nvgpu_readl(g, gr_exception_r()));
	gk20a_debug_output(o, "NV_PGRAPH_ACTIVITY0: 0x%x\n",
		nvgpu_readl(g, gr_activity_0_r()));
	gk20a_debug_output(o, "NV_PGRAPH_ACTIVITY1: 0x%x\n",
		nvgpu_readl(g, gr_activity_1_r()));
	gk20a_debug_output(o, "NV_PGRAPH_ACTIVITY2: 0x%x\n",
		nvgpu_readl(g, gr_activity_2_r()));
	gk20a_debug_output(o, "NV_PGRAPH_ACTIVITY3: 0x%x\n",
		nvgpu_readl(g, gr_activity_3_r()));
	gk20a_debug_output(o, "NV_PGRAPH_ACTIVITY4: 0x%x\n",
		nvgpu_readl(g, gr_activity_4_r()));

	return 0;
}

int gb10b_gr_dump_gr_status_regs(struct gk20a *g,
				 struct nvgpu_debug_context *o)
{
	u32 gr_engine_id, gpc, gpc_instance_offset;
	struct nvgpu_engine_status_info engine_status;
	struct nvgpu_gr *gr = nvgpu_gr_get_cur_instance_ptr(g);

	gr_engine_id = nvgpu_engine_get_gr_id(g);

	gk20a_debug_output(o, "NV_PGRAPH_STATUS: 0x%x\n",
		nvgpu_readl(g, gr_status_r()));
	gk20a_debug_output(o, "NV_PGRAPH_STATUS1: 0x%x\n",
		nvgpu_readl(g, gr_status_1_r()));
	gk20a_debug_output(o, "NV_PGRAPH_INTR: 0x%x\n",
		nvgpu_readl(g, gr_intr_r()));
	gk20a_debug_output(o, "NV_PGRAPH_ENGINE_STATUS: 0x%x\n",
		nvgpu_readl(g, gr_engine_status_r()));
	gk20a_debug_output(o, "NV_PGRAPH_GRFIFO_STATUS : 0x%x\n",
		nvgpu_readl(g, gr_gpfifo_status_r()));
	gk20a_debug_output(o, "NV_PGRAPH_GRFIFO_CONTROL : 0x%x\n",
		nvgpu_readl(g, gr_gpfifo_ctl_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_FECS_HOST_INT_STATUS : 0x%x\n",
		nvgpu_readl(g, gr_fecs_host_int_status_r()));
	gk20a_debug_output(o, "NV_PGRAPH_EXCEPTION  : 0x%x\n",
		nvgpu_readl(g, gr_exception_r()));
	gk20a_debug_output(o, "NV_PGRAPH_FECS_INTR  : 0x%x\n",
		nvgpu_readl(g, gr_fecs_intr_r()));
	g->ops.engine_status.read_engine_status_info(g, gr_engine_id,
		&engine_status);
	gk20a_debug_output(o, "NV_PFIFO_ENGINE_STATUS(GR) : 0x%x\n",
		engine_status.reg_data);
	gk20a_debug_output(o, "NV_PGRAPH_ACTIVITY0: 0x%x\n",
		nvgpu_readl(g, gr_activity_0_r()));
	gk20a_debug_output(o, "NV_PGRAPH_ACTIVITY1: 0x%x\n",
		nvgpu_readl(g, gr_activity_1_r()));
	gk20a_debug_output(o, "NV_PGRAPH_ACTIVITY4: 0x%x\n",
		nvgpu_readl(g, gr_activity_4_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_SKED_ACTIVITY: 0x%x\n",
		nvgpu_readl(g, gr_pri_sked_activity_r()));

	for (gpc = 0U;
	     gpc < nvgpu_gr_config_get_gpc_count(gr->config); gpc++) {
		gpc_instance_offset = nvgpu_gr_gpc_offset(g, gpc);

		gk20a_debug_output(o, "NV_PGRAPH_PRI_GPC%d_GPCCS_GPC_ACTIVITY0: 0x%x\n", gpc,
			nvgpu_readl(g, nvgpu_safe_add_u32(gr_pri_gpc0_gpccs_gpc_activity0_r(),
			gpc_instance_offset)));
		gk20a_debug_output(o, "NV_PGRAPH_PRI_GPC%d_GPCCS_GPC_ACTIVITY1: 0x%x\n", gpc,
			nvgpu_readl(g, nvgpu_safe_add_u32(gr_pri_gpc0_gpccs_gpc_activity1_r(),
			gpc_instance_offset)));
		gk20a_debug_output(o, "NV_PGRAPH_PRI_GPC%d_GPCCS_GPC_ACTIVITY2: 0x%x\n", gpc,
			nvgpu_readl(g, nvgpu_safe_add_u32(gr_pri_gpc0_gpccs_gpc_activity2_r(),
			gpc_instance_offset)));
		gk20a_debug_output(o, "NV_PGRAPH_PRI_GPC%d_GPCCS_GPC_ACTIVITY3: 0x%x\n", gpc,
			nvgpu_readl(g, nvgpu_safe_add_u32(gr_pri_gpc0_gpccs_gpc_activity3_r(),
			gpc_instance_offset)));
		gk20a_debug_output(o, "NV_PGRAPH_PRI_GPC%d_GPCCS_GPC_ACTIVITY4: 0x%x\n", gpc,
			nvgpu_readl(g, nvgpu_safe_add_u32(gr_pri_gpc0_gpccs_gpc_activity4_r(),
			gpc_instance_offset)));
		gk20a_debug_output(o, "NV_PGRAPH_PRI_GPC%d_GPCCS_GPC_ACTIVITY5: 0x%x\n", gpc,
			nvgpu_readl(g, nvgpu_safe_add_u32(gr_pri_gpc0_gpccs_gpc_activity5_r(),
			gpc_instance_offset)));
		gk20a_debug_output(o, "NV_PGRAPH_PRI_GPC%d_GPCCS_GPC_ACTIVITY6: 0x%x\n", gpc,
			nvgpu_readl(g, nvgpu_safe_add_u32(gr_pri_gpc0_gpccs_gpc_activity6_r(),
			gpc_instance_offset)));
	}

	gb10b_gr_dump_tpc_activity_regs(g,o);

	gk20a_debug_output(o, "NV_PGRAPH_PRI_GPCS_GPCCS_GPC_ACTIVITY0: 0x%x\n",
		nvgpu_readl(g, gr_pri_gpcs_gpccs_gpc_activity_0_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_GPCS_GPCCS_GPC_ACTIVITY1: 0x%x\n",
		nvgpu_readl(g, gr_pri_gpcs_gpccs_gpc_activity_1_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_GPCS_GPCCS_GPC_ACTIVITY2: 0x%x\n",
		nvgpu_readl(g, gr_pri_gpcs_gpccs_gpc_activity_2_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_GPCS_GPCCS_GPC_ACTIVITY3: 0x%x\n",
		nvgpu_readl(g, gr_pri_gpcs_gpccs_gpc_activity_3_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_GPCS_GPCCS_GPC_ACTIVITY4: 0x%x\n",
		nvgpu_readl(g, gr_pri_gpcs_gpccs_gpc_activity_4_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_GPCS_GPCCS_GPC_ACTIVITY5: 0x%x\n",
		nvgpu_readl(g, gr_pri_gpcs_gpccs_gpc_activity_5_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_GPCS_GPCCS_GPC_ACTIVITY6: 0x%x\n",
		nvgpu_readl(g, gr_pri_gpcs_gpccs_gpc_activity_6_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_GPCS_TPCS_TPCCS_TPC_ACTIVITY0: 0x%x\n",
		nvgpu_readl(g, gr_pri_gpcs_tpcs_tpccs_tpc_activity_0_r()));
	if (nvgpu_support_gfx_with_numa(g) ||
			nvgpu_grmgr_is_cur_instance_support_gfx(g)) {
		gk20a_debug_output(o, "NV_PGRAPH_PRI_DS_MPIPE_STATUS: 0x%x\n",
			nvgpu_readl(g, gr_pri_ds_mpipe_status_r()));
	}

	gk20a_debug_output(o, "NV_PGRAPH_PRI_FE_HWW_ESR : 0x%x\n",
		nvgpu_readl(g, gr_fe_hww_esr_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_FE_HWW_ESR_INFO : 0x%x\n",
		nvgpu_readl(g, gr_fe_hww_esr_info_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_FE_GO_IDLE_TIMEOUT : 0x%x\n",
		nvgpu_readl(g, gr_fe_go_idle_timeout_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_FE_GO_IDLE_INFO : 0x%x\n",
		nvgpu_readl(g, gr_pri_fe_go_idle_info_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_GPC0_TPC0_TEX_M_TEX_SUBUNITS_STATUS: 0x%x\n",
		nvgpu_readl(g, gr_pri_gpc0_tpc0_tex_m_tex_subunits_status_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_CWD_FS: 0x%x\n",
		nvgpu_readl(g, gr_cwd_fs_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_FE_TPC_FS(0): 0x%x\n",
		nvgpu_readl(g, gr_fe_tpc_fs_r(0)));

	if (g->ops.gr.init.is_cwd_gpc_tpc_id_removed == NULL) {
		gk20a_debug_output(o, "NV_PGRAPH_PRI_CWD_GPC_TPC_ID: 0x%x\n",
			nvgpu_readl(g, gr_cwd_gpc_tpc_id_r(0)));
	}

	gk20a_debug_output(o, "NV_PGRAPH_PRI_CWD_SM_ID(0): 0x%x\n",
		nvgpu_readl(g, gr_cwd_sm_id_r(0)));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_FECS_CTXSW_STATUS_FE_0: 0x%x\n",
		g->ops.gr.falcon.read_fecs_ctxsw_status0(g));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_FECS_CTXSW_STATUS_1: 0x%x\n",
		g->ops.gr.falcon.read_fecs_ctxsw_status1(g));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_GPC0_GPCCS_CTXSW_STATUS_GPC_0: 0x%x\n",
		nvgpu_readl(g, gr_gpc0_gpccs_ctxsw_status_gpc_0_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_GPC0_GPCCS_CTXSW_STATUS_1: 0x%x\n",
		nvgpu_readl(g, gr_gpc0_gpccs_ctxsw_status_1_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_FECS_CTXSW_IDLESTATE : 0x%x\n",
		nvgpu_readl(g, gr_fecs_ctxsw_idlestate_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_GPC0_GPCCS_CTXSW_IDLESTATE : 0x%x\n",
		nvgpu_readl(g, gr_gpc0_gpccs_ctxsw_idlestate_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_FECS_CURRENT_CTX : 0x%x\n",
		g->ops.gr.falcon.get_current_ctx(g));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_FECS_NEW_CTX : 0x%x\n",
		nvgpu_readl(g, gr_fecs_new_ctx_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_FECS_HOST_INT_ENABLE : 0x%x\n",
		nvgpu_readl(g, gr_fecs_host_int_enable_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_FECS_HOST_INT_STATUS : 0x%x\n",
		nvgpu_readl(g, gr_fecs_host_int_status_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_GPCS_ROP0_CROP_STATUS1 : 0x%x\n",
		nvgpu_readl(g, gr_pri_gpcs_rop0_crop_status1_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_GPCS_ROPS_CROP_STATUS1 : 0x%x\n",
		nvgpu_readl(g, gr_pri_gpcs_rops_crop_status1_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_GPCS_ROP0_ZROP_STATUS : 0x%x\n",
		nvgpu_readl(g, gr_pri_gpcs_rop0_zrop_status_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_GPCS_ROP0_ZROP_STATUS2 : 0x%x\n",
		nvgpu_readl(g, gr_pri_gpcs_rop0_zrop_status2_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_GPCS_ROP1_ZROP_STATUS: 0x%x\n",
		nvgpu_readl(g, nvgpu_safe_add_u32(
			gr_pri_gpcs_rop0_zrop_status_r(),
			nvgpu_get_litter_value(g, GPU_LIT_ROP_STRIDE))));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_GPCS_ROP1_ZROP_STATUS2: 0x%x\n",
		nvgpu_readl(g, nvgpu_safe_add_u32(
			gr_pri_gpcs_rop0_zrop_status2_r(),
			nvgpu_get_litter_value(g, GPU_LIT_ROP_STRIDE))));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_GPCS_ROPS_ZROP_STATUS : 0x%x\n",
		nvgpu_readl(g, gr_pri_gpcs_rops_zrop_status_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_GPCS_ROPS_ZROP_STATUS2 : 0x%x\n",
		nvgpu_readl(g, gr_pri_gpcs_rops_zrop_status2_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_GPC0_GPCCS_GPC_EXCEPTION: 0x%x\n",
		nvgpu_readl(g, gr_pri_gpc0_gpccs_gpc_exception_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_GPC0_GPCCS_GPC_EXCEPTION_EN: 0x%x\n",
		nvgpu_readl(g, gr_pri_gpc0_gpccs_gpc_exception_en_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_GPC0_TPC0_TPCCS_TPC_EXCEPTION: 0x%x\n",
		nvgpu_readl(g, gr_pri_gpc0_tpc0_tpccs_tpc_exception_r()));
	gk20a_debug_output(o, "NV_PGRAPH_PRI_GPC0_TPC0_TPCCS_TPC_EXCEPTION_EN: 0x%x\n",
		nvgpu_readl(g, gr_pri_gpc0_tpc0_tpccs_tpc_exception_en_r()));

	gb10b_gr_dump_gr_sm_regs(g, o);

	return 0;
}

void gb10b_gr_set_circular_buffer_size(struct gk20a *g, u32 data)
{
	struct nvgpu_gr *gr = nvgpu_gr_get_cur_instance_ptr(g);
	u32 gpc_index, ppc_index, stride_ppc, ppc, stride, val, ppc_reg;
	u32 cb_size_steady = nvgpu_safe_mult_u32(data, 4U), cb_size;
	u32 attrib_cb_size = g->ops.gr.init.get_attrib_cb_size(g,
		nvgpu_gr_config_get_tpc_count(gr->config));
	u32 ppc_in_gpc_stride = g->ops.get_litter_value(g, GPU_LIT_PPC_IN_GPC_STRIDE);

	nvgpu_log_fn(g, " ");

	if (cb_size_steady > attrib_cb_size) {
		cb_size_steady = attrib_cb_size;
	}

	if (nvgpu_readl(g, gr_gpc0_ppc0_cbm_beta_cb_size_r()) !=
		nvgpu_readl(g,
			gr_gpc0_ppc0_cbm_beta_steady_state_cb_size_r())) {

		u32 cbm_beta_cb_size = nvgpu_safe_sub_u32(
					g->ops.gr.init.get_attrib_cb_gfxp_default_size(g),
					g->ops.gr.init.get_attrib_cb_default_size(g));

		cb_size = nvgpu_safe_add_u32(cb_size_steady, cbm_beta_cb_size);
	} else {
		cb_size = cb_size_steady;
	}

	nvgpu_writel(g, gr_ds_tga_constraintlogic_beta_r(),
		(nvgpu_readl(g, gr_ds_tga_constraintlogic_beta_r()) &
		 ~gr_ds_tga_constraintlogic_beta_cbsize_f(~U32(0U))) |
		 gr_ds_tga_constraintlogic_beta_cbsize_f(cb_size_steady));

	for (gpc_index = 0;
	     gpc_index < nvgpu_gr_config_get_gpc_count(gr->config);
	     gpc_index++) {
		stride = nvgpu_safe_mult_u32(g->ops.get_litter_value(g, GPU_LIT_GPC_STRIDE),
				gpc_index);

		for (ppc_index = 0;
		     ppc_index < nvgpu_gr_config_get_gpc_ppc_count(gr->config, gpc_index);
		     ppc_index++) {

			ppc = nvgpu_safe_mult_u32(ppc_in_gpc_stride, ppc_index);
			stride_ppc = nvgpu_safe_add_u32(stride, ppc);
			ppc_reg = nvgpu_safe_add_u32(gr_gpc0_ppc0_cbm_beta_cb_size_r(),
					stride_ppc);

			val = nvgpu_readl(g, ppc_reg);
			val = set_field(val,
				gr_gpc0_ppc0_cbm_beta_cb_size_v_m(),
				gr_gpc0_ppc0_cbm_beta_cb_size_v_f(nvgpu_safe_mult_u32(cb_size,
					nvgpu_gr_config_get_pes_tpc_count(gr->config,
						gpc_index, ppc_index))));

			nvgpu_writel(g, ppc_reg, val);

			nvgpu_writel(g, nvgpu_safe_add_u32(nvgpu_safe_add_u32(
				(nvgpu_safe_mult_u32(ppc_in_gpc_stride, ppc_index)),
				gr_gpc0_ppc0_cbm_beta_steady_state_cb_size_r()), stride),
				gr_gpc0_ppc0_cbm_beta_steady_state_cb_size_v_f(cb_size_steady));

			val = nvgpu_readl(g, gr_gpcs_swdx_tc_beta_cb_size_r(
					nvgpu_safe_add_u32(ppc_index, gpc_index)));

			val = set_field(val,
				gr_gpcs_swdx_tc_beta_cb_size_v_m(),
				gr_gpcs_swdx_tc_beta_cb_size_v_f(
					nvgpu_safe_mult_u32(cb_size_steady,
					nvgpu_gr_config_get_gpc_ppc_count(gr->config, gpc_index))));

			nvgpu_writel(g, gr_gpcs_swdx_tc_beta_cb_size_r(
				nvgpu_safe_add_u32(ppc_index, gpc_index)), val);
		}
	}
}

static void gb10b_gr_sm_dump_warp_bpt_pause_trap_mask_regs(struct gk20a *g,
					u32 offset, bool timeout)
{
	u64 warps_valid = 0, warps_paused = 0, warps_trapped = 0;
	u32 dbgr_control0 = nvgpu_readl(g,
			nvgpu_safe_add_u32(gr_gpc0_tpc0_sm0_dbgr_control0_r(), offset));
	u32 dbgr_status0 = nvgpu_readl(g,
			nvgpu_safe_add_u32(gr_gpc0_tpc0_sm0_dbgr_status0_r(), offset));
	/* 64 bit read */
	warps_valid =
		(u64)nvgpu_readl(g, nvgpu_safe_add_u32(
				gr_gpc0_tpc0_sm0_warp_valid_mask_1_r(), offset)) << 32;
	warps_valid |= nvgpu_readl(g, nvgpu_safe_add_u32(
				gr_gpc0_tpc0_sm0_warp_valid_mask_0_r(), offset));

	/* 64 bit read */
	warps_paused =
		(u64)nvgpu_readl(g, nvgpu_safe_add_u32(
				gr_gpc0_tpc0_sm0_dbgr_bpt_pause_mask_1_r(), offset)) << 32;
	warps_paused |= nvgpu_readl(g, nvgpu_safe_add_u32(
				gr_gpc0_tpc0_sm0_dbgr_bpt_pause_mask_0_r(), offset));

	/* 64 bit read */
	warps_trapped =
		(u64)nvgpu_readl(g,
			nvgpu_safe_add_u32(gr_gpc0_tpc0_sm0_dbgr_bpt_trap_mask_1_r(), offset)) << 32;
	warps_trapped |= nvgpu_readl(g,
			nvgpu_safe_add_u32(gr_gpc0_tpc0_sm0_dbgr_bpt_trap_mask_0_r(), offset));
	if (timeout) {
		nvgpu_err(g,
			  "STATUS0=0x%x CONTROL0=0x%x VALID_MASK=0x%llx "
			  "PAUSE_MASK=0x%llx TRAP_MASK=0x%llx",
			  dbgr_status0, dbgr_control0, warps_valid,
			  warps_paused, warps_trapped);
	} else {
		nvgpu_log(g, gpu_dbg_intr | gpu_dbg_gpu_dbg,
			  "STATUS0=0x%x CONTROL0=0x%x VALID_MASK=0x%llx "
			  "PAUSE_MASK=0x%llx TRAP_MASK=0x%llx",
			  dbgr_status0, dbgr_control0, warps_valid,
			  warps_paused, warps_trapped);
	}
}

int gb10b_gr_wait_for_sm_lock_down(struct gk20a *g,
		u32 gpc, u32 tpc, u32 sm,
		u32 global_esr_mask, bool check_errors)
{
	bool locked_down;
	bool no_error_pending;
	u32 delay = POLL_DELAY_MIN_US;
	u32 reg = 0U;
#ifdef CONFIG_NVGPU_REPLAYABLE_FAULT
	bool mmu_debug_mode_enabled = g->ops.fb.is_debug_mode_enabled(g);
#endif
	u32 dbgr_status0 = 0;
	u32 warp_esr, global_esr;
	struct nvgpu_timeout timeout;
	u32 offset = nvgpu_gr_gpc_offset(g, gpc) +
			nvgpu_gr_tpc_offset(g, tpc) +
			nvgpu_gr_sm_offset(g, sm);

	nvgpu_log(g, gpu_dbg_intr | gpu_dbg_gpu_dbg,
		"GPC%d TPC%d: locking down SM%d", gpc, tpc, sm);

	nvgpu_timeout_init_cpu_timer(g, &timeout, g->poll_timeout_default);

	/* wait for the sm to lock down */
	do {
		global_esr = g->ops.gr.intr.get_sm_hww_global_esr(g, gpc, tpc, sm);

		if (nvgpu_safe_add_u32_return(
			gr_gpc0_tpc0_sm0_dbgr_status0_r(), offset, &reg) == false) {
			nvgpu_err(g, "buffer overflow");
			return -EOVERFLOW;
		}
		dbgr_status0 = nvgpu_readl(g, reg);

		warp_esr = g->ops.gr.intr.get_sm_hww_warp_esr(g, gpc, tpc, sm);

		locked_down =
		    (gr_gpc0_tpc0_sm0_dbgr_status0_locked_down_v(dbgr_status0) ==
		     gr_gpc0_tpc0_sm0_dbgr_status0_locked_down_true_v());
		no_error_pending =
			check_errors &&
			(gr_gpc0_tpc0_sm0_hww_warp_esr_error_v(warp_esr) ==
			 gr_gpc0_tpc0_sm0_hww_warp_esr_error_none_v()) &&
			((global_esr & global_esr_mask) == 0U);

		if (locked_down) {
		/*
		 * if SM reports locked down, it means that SM is idle and
		 * trapped and also that one of the these conditions are true
		 * 1) sm is nonempty and all valid warps are paused
		 * 2) sm is empty and held in trapped state due to stop trigger
		 * 3) sm is nonempty and some warps are not paused, but are
		 *    instead held at RTT due to an "active" stop trigger
		 * Check for Paused warp mask != Valid
		 * warp mask after SM reports it is locked down in order to
		 * distinguish case 1 from case 3.  When case 3 is detected,
		 * it implies a misprogrammed trap handler code, as all warps
		 * in the handler must promise to BPT.PAUSE instead of RTT
		 * whenever SR64 read in trap mode indicates stop trigger
		 * is asserted.
		 */
			gb10b_gr_sm_dump_warp_bpt_pause_trap_mask_regs(g,
						offset, false);
		}

		if (locked_down || no_error_pending) {
			nvgpu_log(g, gpu_dbg_intr | gpu_dbg_gpu_dbg,
				"GPC%d TPC%d: locked down SM%d", gpc, tpc, sm);
			return 0;
		}
#ifdef CONFIG_NVGPU_REPLAYABLE_FAULT
		if (mmu_debug_mode_enabled &&
		    g->ops.fb.handle_replayable_fault != NULL) {
			g->ops.fb.handle_replayable_fault(g);
		} else {
#endif
			/* if an mmu fault is pending and mmu debug mode is not
			 * enabled, the sm will never lock down.
			 */
			if (g->ops.gin.is_mmu_fault_pending(g)) {
				nvgpu_err(g,
					"GPC%d TPC%d: mmu fault pending,"
					" SM%d will never lock down!",
					gpc, tpc, sm);
				return -EFAULT;
			}
#ifdef CONFIG_NVGPU_REPLAYABLE_FAULT
		}
#endif

		nvgpu_usleep_range(delay, delay * 2U);
		delay = min_t(u32, delay << 1, POLL_DELAY_MAX_US);
	} while (nvgpu_timeout_expired(&timeout) == 0);

	nvgpu_err(g, "GPC%d TPC%d: timed out while trying to "
			"lock down SM%d", gpc, tpc, sm);
	gb10b_gr_sm_dump_warp_bpt_pause_trap_mask_regs(g, offset, true);

	return -ETIMEDOUT;
}

void gb10b_gr_bpt_reg_info(struct gk20a *g, struct nvgpu_warpstate *w_state)
{
	/* Check if we have at least one valid warp
	 * get paused state.
	 */
	struct nvgpu_gr *gr = nvgpu_gr_get_cur_instance_ptr(g);
	u32 gpc, tpc, sm, sm_id;
	u32 offset;
	u64 warps_valid = 0, warps_paused = 0, warps_trapped = 0;
	u32 no_of_sm = nvgpu_gr_config_get_no_of_sm(gr->config);

	for (sm_id = 0; sm_id < no_of_sm; sm_id++) {
		struct nvgpu_sm_info *sm_info =
			nvgpu_gr_config_get_sm_info(gr->config, sm_id);
		gpc = nvgpu_gr_config_get_sm_info_gpc_index(sm_info);
		tpc = nvgpu_gr_config_get_sm_info_tpc_index(sm_info);
		sm = nvgpu_gr_config_get_sm_info_sm_index(sm_info);

		offset = nvgpu_gr_gpc_offset(g, gpc) +
			 nvgpu_gr_tpc_offset(g, tpc) +
			 nvgpu_gr_sm_offset(g, sm);

		/* 64 bit read */
		warps_valid = (u64)nvgpu_readl(g,
				gr_gpc0_tpc0_sm0_warp_valid_mask_1_r() +
				offset) << 32;
		warps_valid |= nvgpu_readl(g,
				gr_gpc0_tpc0_sm0_warp_valid_mask_0_r() +
				offset);

		/* 64 bit read */
		warps_paused = (u64)nvgpu_readl(g,
				gr_gpc0_tpc0_sm0_dbgr_bpt_pause_mask_1_r() +
				offset) << 32;
		warps_paused |= nvgpu_readl(g,
				gr_gpc0_tpc0_sm0_dbgr_bpt_pause_mask_0_r() +
				offset);

		/* 64 bit read */
		warps_trapped = (u64)nvgpu_readl(g, nvgpu_safe_add_u32(
				gr_gpc0_tpc0_sm0_dbgr_bpt_trap_mask_1_r(), offset)) << 32;
		warps_trapped |= nvgpu_readl(g,
				gr_gpc0_tpc0_sm0_dbgr_bpt_trap_mask_0_r() +
				offset);

		w_state[sm_id].valid_warps[0] = warps_valid;
		w_state[sm_id].trapped_warps[0] = warps_trapped;
		w_state[sm_id].paused_warps[0] = warps_paused;
	}


	/* Only for debug purpose */
	for (sm_id = 0; sm_id < no_of_sm; sm_id++) {
		nvgpu_log_fn(g, "w_state[%d].valid_warps[0]: %llx",
					sm_id, w_state[sm_id].valid_warps[0]);
		nvgpu_log_fn(g, "w_state[%d].valid_warps[1]: %llx",
					sm_id, w_state[sm_id].valid_warps[1]);

		nvgpu_log_fn(g, "w_state[%d].trapped_warps[0]: %llx",
					sm_id, w_state[sm_id].trapped_warps[0]);
		nvgpu_log_fn(g, "w_state[%d].trapped_warps[1]: %llx",
					sm_id, w_state[sm_id].trapped_warps[1]);

		nvgpu_log_fn(g, "w_state[%d].paused_warps[0]: %llx",
					sm_id, w_state[sm_id].paused_warps[0]);
		nvgpu_log_fn(g, "w_state[%d].paused_warps[1]: %llx",
					sm_id, w_state[sm_id].paused_warps[1]);
	}
}

void gb10b_gr_set_gpcs_rops_crop_debug4(struct gk20a *g, u32 data)
{
	u32 val;

	nvgpu_log_fn(g, " ");

	val = nvgpu_readl(g, gr_pri_gpcs_rops_crop_debug4_r());
	if ((data & gr_pri_gpcs_rops_crop_debug4_clamp_fp_blend_s()) ==
		gr_pri_gpcs_rops_crop_debug4_clamp_fp_blend_to_maxval_v()) {
		val = set_field(val,
			gr_pri_gpcs_rops_crop_debug4_clamp_fp_blend_m(),
			gr_pri_gpcs_rops_crop_debug4_clamp_fp_blend_to_maxval_f());
	} else if ((data & gr_pri_gpcs_rops_crop_debug4_clamp_fp_blend_s()) ==
		gr_pri_gpcs_rops_crop_debug4_clamp_fp_blend_to_inf_v()) {
		val = set_field(val,
			gr_pri_gpcs_rops_crop_debug4_clamp_fp_blend_m(),
			gr_pri_gpcs_rops_crop_debug4_clamp_fp_blend_to_inf_f());
	} else {
		nvgpu_warn(g,
			"wrong data sent for crop_debug4: %x08x", data);
		return;
	}
	nvgpu_writel(g, gr_pri_gpcs_rops_crop_debug4_r(), val);
}

#ifdef CONFIG_NVGPU_HAL_NON_FUSA
void gb10b_gr_vab_reserve(struct gk20a *g, u32 vab_reg, u32 num_range_checkers,
	struct nvgpu_vab_range_checker *vab_range_checker)
{
	/*
	 * configure range checkers in GPC
	 */

	u32 i = 0U;
	u32 granularity_shift_bits_base = 16U; /* log(64KB) */
	u32 granularity_shift_bits = 0U;

	nvgpu_log_fn(g, " ");

	for (i = 0U; i < num_range_checkers; i++) {
		granularity_shift_bits = nvgpu_safe_sub_u32(
			vab_range_checker[i].granularity_shift,
			granularity_shift_bits_base);

		nvgpu_writel(g, gr_gpcs_mmu_vidmem_access_bit_start_addr_hi_r(i),
			U32(vab_range_checker[i].start_phys_addr >> 32U));

		nvgpu_writel(g, gr_gpcs_mmu_vidmem_access_bit_start_addr_lo_r(i),
			(u32)(vab_range_checker[i].start_phys_addr &
			gr_gpcs_mmu_vidmem_access_bit_start_addr_lo_val_m()) |
			gr_gpcs_mmu_vidmem_access_bit_start_addr_lo_granularity_f(
				granularity_shift_bits));
	}

	/* Setup VAB */
	nvgpu_writel(g, gr_gpcs_mmu_vidmem_access_bit_r(), vab_reg);
}

void gb10b_gr_vab_configure(struct gk20a *g, u32 vab_reg)
{
	nvgpu_writel(g, gr_gpcs_mmu_vidmem_access_bit_r(), vab_reg);
}
#endif /* CONFIG_NVGPU_HAL_NON_FUSA */

#ifdef CONFIG_NVGPU_DEBUGGER
int gb10b_gr_set_sched_wait_for_errbar(struct gk20a *g,
	struct nvgpu_channel *ch, bool enable)
{
	struct nvgpu_dbg_reg_op ctx_ops = {
		.op = REGOP(WRITE_32),
		.type = REGOP(TYPE_GR_CTX),
		.offset = gr_gpcs_pri_tpcs_sm_sch_macro_sched_r(),
		.value_lo = enable ?
		gr_gpcs_pri_tpcs_sm_sch_macro_sched_exit_wait_for_errbar_enabled_f() :
		gr_gpcs_pri_tpcs_sm_sch_macro_sched_exit_wait_for_errbar_disabled_f(),
	};
	int err;
	struct nvgpu_tsg *tsg = nvgpu_tsg_from_ch(ch);
	u32 flags = NVGPU_REG_OP_FLAG_MODE_ALL_OR_NONE;

	if (tsg != NULL) {
		err = g->ops.regops.exec_regops(g, tsg, &ctx_ops, 1, 1, 0, &flags);
		if (err != 0) {
			nvgpu_err(g, "update implicit ERRBAR failed");
		}
	} else {
		nvgpu_err(g, "chid: %d is not bound to tsg", ch->chid);
		return -EINVAL;
	}
	return err;
}

u32 gb10b_gr_get_egpc_base(struct gk20a *g)
{
	(void)g;
	// From blackwell onwards, EGPC/ETPC pri space has been merged into
	// GPC/TPC pri space.
	return nvgpu_get_litter_value(g, GPU_LIT_GPC_BASE);
}

u32 gb10b_gr_get_egpc_shared_base(struct gk20a *g)
{
	(void)g;
	return nvgpu_get_litter_value(g, GPU_LIT_GPC_SHARED_BASE);
}

int gb10b_gr_find_priv_offset_in_buffer(struct gk20a *g, u32 addr,
				u32 *context_buffer, u32 context_buffer_size,
				u32 max_offsets, u32 *priv_offset,
				u32 *num_offsets)
{
	struct nvgpu_gr *gr = nvgpu_gr_get_cur_instance_ptr((g));
	struct nvgpu_priv_addr_map *local_p_map = gr->pri_addr_map;
	int err = -1;
	(void)context_buffer;
	(void)context_buffer_size;

	nvgpu_log_fn(g, " ");

	if(local_p_map->init_done) {
		err = nvgpu_gr_search_pri_addr_map(g, local_p_map, addr,
				max_offsets, priv_offset, num_offsets);
		nvgpu_log(g, gpu_dbg_gpu_dbg,
				"Addr - 0x%x not found in the PRI MAP", addr);
	}

	return  err;
}

void gb10b_gr_disable_smpc(struct gk20a *g)
{
	nvgpu_writel(g, gr_gpcs_tpcs_sm_dsm_perf_counter_control_r(), 0U);
	nvgpu_writel(g, gr_gpcs_tpcs_sm_dsm_perf_counter_control0_r(), 0U);
	nvgpu_writel(g, gr_gpcs_tpcs_sm_dsm_perf_counter_control5_r(), 0U);

	if (g->ops.priv_ring.read_pri_fence != NULL) {
		g->ops.priv_ring.read_pri_fence(g);
	}
}

int gr_gb10b_decode_priv_addr(struct gk20a *g, u32 addr,
	enum ctxsw_addr_type *addr_type,
	u32 *gpc_num, u32 *tpc_num, u32 *ppc_num, u32 *be_num,
	u32 *broadcast_flags)
{
	/**
	 * Special handling for registers under: ctx_reg_LRCC
	 */
	if (g->ops.lrc.is_lrc_supported && g->ops.lrc.pri_is_lrcc_addr(g, addr)) {
		*addr_type = CTXSW_ADDR_TYPE_LRCC;
		if (g->ops.lrc.pri_is_lrcs_lrccs_addr(g, addr)) {
			*broadcast_flags |= PRI_BROADCAST_FLAGS_LRCCS;
		} else if (g->ops.lrc.pri_is_lrcs_addr(g, addr)) {
			*broadcast_flags |= PRI_BROADCAST_FLAGS_LRCS;
		}
		return 0;
	}

	return gr_ga10b_decode_priv_addr(g, addr, addr_type, gpc_num,
			tpc_num, ppc_num, be_num, broadcast_flags);
}

int gr_gb10b_create_priv_addr_table(struct gk20a *g, u32 addr,
		u32 *priv_addr_table, u32 *num_registers)
{
	enum ctxsw_addr_type addr_type;
	u32 gpc_num = 0U, tpc_num = 0U, ppc_num = 0U, be_num = 0U;
	u32 broadcast_flags = 0U;
	u32 index;
	int err;

	index = 0U;
	*num_registers = 0U;

	nvgpu_log(g, gpu_dbg_gpu_dbg, "addr=0x%x", addr);

	err = g->ops.gr.decode_priv_addr(g, addr, &addr_type,
			&gpc_num, &tpc_num, &ppc_num, &be_num,
			&broadcast_flags);
	if (err != 0) {
		return err;
	}

	if (addr_type == CTXSW_ADDR_TYPE_LRCC && g->ops.lrc.is_lrc_supported) {
		if (broadcast_flags & PRI_BROADCAST_FLAGS_LRCCS) {
			g->ops.lrc.split_lrcc_broadcast_addr(g, addr,
					priv_addr_table, &index);
		} else if (broadcast_flags & PRI_BROADCAST_FLAGS_LRCS) {
			g->ops.lrc.split_lrc_broadcast_addr(g, addr,
					priv_addr_table, &index);
		}
		*num_registers = index;
		return 0;
	}

	return gr_ga10b_create_priv_addr_table(g, addr, priv_addr_table,
			num_registers);
}
#endif /* CONFIG_NVGPU_DEBUGGER */

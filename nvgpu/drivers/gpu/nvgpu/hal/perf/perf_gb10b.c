// SPDX-License-Identifier: GPL-2.0-only OR MIT
// SPDX-FileCopyrightText: Copyright (c) 2022-2025, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

#include <nvgpu/io.h>
#include <nvgpu/mm.h>
#include <nvgpu/fbp.h>
#include <nvgpu/gr/gr_utils.h>
#include <nvgpu/gr/config.h>
#include <nvgpu/gr/gr_instances.h>
#include <nvgpu/bug.h>
#include <nvgpu/gk20a.h>
#include <nvgpu/utils.h>
#include <nvgpu/kmem.h>
#include <nvgpu/grmgr.h>
#include <nvgpu/soc.h>
#include <nvgpu/gr/gr_instances.h>
#include <nvgpu/string.h>
#include <nvgpu/profiler.h>

#include "perf_gb10b.h"
#include <hal/gr/gr/gr_gb10b.h>
#include <nvgpu/hw/gb10b/hw_perf_gb10b.h>
#include <nvgpu/hw/gb10b/hw_gr_gb10b.h>
#include <nvgpu/hw/gb10b/hw_proj_gb10b.h>

#define PMM_ROUTER_USER_DGMAP_STATUS_SECURE_REG_SIZE_IN_BITS	(32U)
#define pmasys_cblock_instances_count			3U
#define pmasys_channel_instances_count_per_cblock	2U

#define PMA_CBLOCK_IDX(pma_channel_id)		(pma_channel_id) /	 \
			(g->ops.perf.get_pma_channels_per_cblock())
#define PMA_CBLOCK_CH_IDX(pma_channel_id)	(pma_channel_id) %	 \
			(g->ops.perf.get_pma_channels_per_cblock())

#define NV_PERF_SYS_PARTITION_ALL_DG	({\
		(1U << perf_sys_streaming_dg_id_pcie0_v())  |    \
		(1U << perf_sys_streaming_dg_id_pwr0_v())   |    \
		(1U << perf_sys_streaming_dg_id_smca0_v())  |    \
		(1U << perf_sys_streaming_dg_id_smca1_v())  |    \
		(1U << perf_sys_streaming_dg_id_smcb0_v())  |    \
		(1U << perf_sys_streaming_dg_id_smcb1_v())  |    \
		(1U << perf_sys_streaming_dg_id_smga0_v())  |    \
		(1U << perf_sys_streaming_dg_id_sys0_v())   |    \
		(1U << perf_sys_streaming_dg_id_sys1_v())   |    \
		(1U << perf_sys_streaming_dg_id_sys2_v())   |    \
		(1U << perf_sys_streaming_dg_id_xbar0_v());\
		})

#define NV_PERF_SYS_PARTITION_NSR	({\
		(1U << perf_sys_streaming_dg_id_smca0_v())  |    \
		(1U << perf_sys_streaming_dg_id_smcb0_v())  |    \
		(1U << perf_sys_streaming_dg_id_smga0_v());	 \
		})


#define NV_PERF_SYS_PARTITION_SR	({\
		(1U << perf_sys_streaming_dg_id_smca1_v())  |    \
		(1U << perf_sys_streaming_dg_id_smcb1_v());	 \
		})

#define NV_PERF_SYS_PARTITION_ALL_HEMS	({\
		(1U << perf_sys_streaming_dg_id_hemcwd0_v()) |   \
		(1U << perf_sys_streaming_dg_id_hemcwd1_v());\
		})

#define NV_PERF_SYS_PARTITION_HEMCWD0	(1U << perf_sys_streaming_dg_id_hemcwd0_v())

#define NV_PERF_SYS_PARTITION_HEMCWD1	(1U << perf_sys_streaming_dg_id_hemcwd1_v())



static const u32 hwpm_sys_perfmon_regs[] =
{
	/* This list is autogenerated. Do not edit. */
	0x00240000,
	0x00240004,
	0x00240008,
	0x0024000c,
	0x00240010,
	0x00240014,
	0x00240020,
	0x00240024,
	0x00240028,
	0x0024002c,
	0x00240030,
	0x00240034,
	0x00240040,
	0x00240044,
	0x00240048,
	0x0024004c,
	0x00240050,
	0x00240054,
	0x00240058,
	0x0024005c,
	0x00240060,
	0x00240064,
	0x00240068,
	0x0024006c,
	0x00240070,
	0x00240074,
	0x00240078,
	0x0024007c,
	0x00240080,
	0x00240084,
	0x00240088,
	0x0024008c,
	0x00240090,
	0x00240098,
	0x0024009c,
	0x002400a0,
	0x002400a4,
	0x002400a8,
	0x002400ac,
	0x002400b0,
	0x002400b4,
	0x002400b8,
	0x002400bc,
	0x002400c0,
	0x002400c4,
	0x002400c8,
	0x002400cc,
	0x002400d0,
	0x002400d4,
	0x002400d8,
	0x002400dc,
	0x002400e0,
	0x002400e4,
	0x002400e8,
	0x002400ec,
	0x002400f8,
	0x002400fc,
	0x00240108,
	0x00240110,
	0x00240128,
	0x00240114,
	0x00240118,
	0x0024011c,
	0x00240124,
	0x00240130,
	0x00240100,
};

static const u32 hwpm_gpc_perfmon_regs[] =
{
	/* This list is autogenerated. Do not edit. */
	0x00180000,
	0x00180004,
	0x00180008,
	0x0018000c,
	0x00180010,
	0x00180014,
	0x00180020,
	0x00180024,
	0x00180028,
	0x0018002c,
	0x00180030,
	0x00180034,
	0x00180040,
	0x00180044,
	0x00180048,
	0x0018004c,
	0x00180050,
	0x00180054,
	0x00180058,
	0x0018005c,
	0x00180060,
	0x00180064,
	0x00180068,
	0x0018006c,
	0x00180070,
	0x00180074,
	0x00180078,
	0x0018007c,
	0x00180080,
	0x00180084,
	0x00180088,
	0x0018008c,
	0x00180090,
	0x00180098,
	0x0018009c,
	0x001800a0,
	0x001800a4,
	0x001800a8,
	0x001800ac,
	0x001800b0,
	0x001800b4,
	0x001800b8,
	0x001800bc,
	0x001800c0,
	0x001800c4,
	0x001800c8,
	0x001800cc,
	0x001800d0,
	0x001800d4,
	0x001800d8,
	0x001800dc,
	0x001800e0,
	0x001800e4,
	0x001800e8,
	0x001800ec,
	0x001800f8,
	0x001800fc,
	0x00180108,
	0x00180110,
	0x00180128,
	0x00180114,
	0x00180118,
	0x0018011c,
	0x00180124,
	0x00180130,
	0x00180100,
};

static const u32 hwpm_fbp_perfmon_regs[] =
{
	/* This list is autogenerated. Do not edit. */
	0x00200000,
	0x00200004,
	0x00200008,
	0x0020000c,
	0x00200010,
	0x00200014,
	0x00200020,
	0x00200024,
	0x00200028,
	0x0020002c,
	0x00200030,
	0x00200034,
	0x00200040,
	0x00200044,
	0x00200048,
	0x0020004c,
	0x00200050,
	0x00200054,
	0x00200058,
	0x0020005c,
	0x00200060,
	0x00200064,
	0x00200068,
	0x0020006c,
	0x00200070,
	0x00200074,
	0x00200078,
	0x0020007c,
	0x00200080,
	0x00200084,
	0x00200088,
	0x0020008c,
	0x00200090,
	0x00200098,
	0x0020009c,
	0x002000a0,
	0x002000a4,
	0x002000a8,
	0x002000ac,
	0x002000b0,
	0x002000b4,
	0x002000b8,
	0x002000bc,
	0x002000c0,
	0x002000c4,
	0x002000c8,
	0x002000cc,
	0x002000d0,
	0x002000d4,
	0x002000d8,
	0x002000dc,
	0x002000e0,
	0x002000e4,
	0x002000e8,
	0x002000ec,
	0x002000f8,
	0x002000fc,
	0x00200108,
	0x00200110,
	0x00200128,
	0x00200114,
	0x00200118,
	0x0020011c,
	0x00200124,
	0x00200130,
	0x00200100,
};

u32 gb10b_perf_get_pma_cblock_instance_count(void)
{
	return pmasys_cblock_instances_count;
}

u32 gb10b_perf_get_pma_channel_count(struct gk20a *g)
{
	return nvgpu_safe_mult_u32(g->ops.perf.get_pma_cblock_instance_count(),
				   g->ops.perf.get_pma_channels_per_cblock());
}

u32 gb10b_perf_get_pma_channels_per_cblock(void)
{
	return pmasys_channel_instances_count_per_cblock;
}

const u32 *gb10b_perf_get_hwpm_sys_perfmon_regs(u32 *count)
{
	*count = sizeof(hwpm_sys_perfmon_regs) / sizeof(hwpm_sys_perfmon_regs[0]);
	return hwpm_sys_perfmon_regs;
}

const u32 *gb10b_perf_get_hwpm_gpc_perfmon_regs(u32 *count)
{
	*count = sizeof(hwpm_gpc_perfmon_regs) / sizeof(hwpm_gpc_perfmon_regs[0]);
	return hwpm_gpc_perfmon_regs;
}

const u32 *gb10b_perf_get_hwpm_fbp_perfmon_regs(u32 *count)
{
	*count = sizeof(hwpm_fbp_perfmon_regs) / sizeof(hwpm_fbp_perfmon_regs[0]);
	return hwpm_fbp_perfmon_regs;
}

bool gb10b_perf_get_membuf_overflow_status(struct gk20a *g, u32 pma_channel_id)
{
	const u32 st =
		perf_pmasys_channel_status_membuf_status_overflowed_f();

	nvgpu_assert(pma_channel_id < g->ops.perf.get_pma_channel_count(g));

	return st == (nvgpu_readl(g,
			perf_pmasys_channel_status_r(PMA_CBLOCK_IDX(pma_channel_id),
						     PMA_CBLOCK_CH_IDX(pma_channel_id))) & st);
}

u32 gb10b_perf_get_membuf_pending_bytes(struct gk20a *g, u32 pma_channel_id)
{
	nvgpu_assert(pma_channel_id < g->ops.perf.get_pma_channel_count(g));

	return nvgpu_readl(g,
		perf_pmasys_channel_mem_bytes_r(PMA_CBLOCK_IDX(pma_channel_id),
						PMA_CBLOCK_CH_IDX(pma_channel_id)));
}

void gb10b_perf_set_membuf_handled_bytes(struct gk20a *g, u32 pma_channel_id,
					 u32 entries, u32 entry_size)
{
	nvgpu_assert(pma_channel_id < g->ops.perf.get_pma_channel_count(g));

	if (entries > 0U) {
		nvgpu_writel(g,
			perf_pmasys_channel_mem_bump_r(PMA_CBLOCK_IDX(pma_channel_id),
						       PMA_CBLOCK_CH_IDX(pma_channel_id)),
			entries * entry_size);
	}
}

void gb10b_perf_membuf_reset_streaming(struct gk20a *g, u32 pma_channel_id)
{
	u32 num_unread_bytes;
	u32 cblock_idx = PMA_CBLOCK_IDX(pma_channel_id);
	u32 channel_idx = PMA_CBLOCK_CH_IDX(pma_channel_id);

	nvgpu_assert(pma_channel_id < g->ops.perf.get_pma_channel_count(g));

	nvgpu_writel(g,
		perf_pmasys_channel_control_user_r(cblock_idx, channel_idx),
		perf_pmasys_channel_control_user_membuf_clear_status_doit_f());

	num_unread_bytes = nvgpu_readl(g,
		perf_pmasys_channel_mem_bytes_r(cblock_idx, channel_idx));
	if (num_unread_bytes != 0U) {
		nvgpu_writel(g,
			perf_pmasys_channel_mem_bump_r(cblock_idx, channel_idx),
			num_unread_bytes);
	}
}

void gb10b_perf_enable_membuf(struct gk20a *g, u32 pma_channel_id, u32 size,
			      u64 buf_addr)
{
	u32 cblock_idx = PMA_CBLOCK_IDX(pma_channel_id);
	u32 channel_idx = PMA_CBLOCK_CH_IDX(pma_channel_id);
	u32 addr_hi, addr_lo;

	nvgpu_assert(pma_channel_id < g->ops.perf.get_pma_channel_count(g));

	addr_lo = u64_lo32(buf_addr);
	addr_hi = u64_hi32(buf_addr);

	nvgpu_writel(g,
		perf_pmasys_channel_outbase_r(cblock_idx, channel_idx), addr_lo);
	nvgpu_writel(g,
		perf_pmasys_channel_outbaseupper_r(cblock_idx, channel_idx),
		perf_pmasys_channel_outbaseupper_ptr_f(addr_hi));
	nvgpu_writel(g,
		perf_pmasys_channel_outsize_r(cblock_idx, channel_idx), size);
}

void gb10b_perf_disable_membuf(struct gk20a *g, u32 pma_channel_id)
{
	gb10b_perf_enable_membuf(g, pma_channel_id, 0U, (u64)0U);
}

void gb10b_perf_bind_mem_bytes_buffer_addr(struct gk20a *g, u32 pma_channel_id,
					   u64 buf_addr)
{
	u32 cblock_idx = PMA_CBLOCK_IDX(pma_channel_id);
	u32 channel_idx = PMA_CBLOCK_CH_IDX(pma_channel_id);
	u32 addr_lo;

	nvgpu_assert(pma_channel_id < g->ops.perf.get_pma_channel_count(g));

	/*
	 * For mem bytes addr, the upper 8 bits of the 40bit VA is taken
	 * from perf_pmasys_channel_outbaseupper_r(), so only consider
	 * the lower 32bits in the buf_addr and discard the rest.
	 */
	buf_addr = u64_lo32(buf_addr);
	buf_addr = buf_addr >> perf_pmasys_channel_mem_bytes_addr_ptr_b();
	addr_lo = nvgpu_safe_cast_u64_to_u32(buf_addr);

	nvgpu_writel(g, perf_pmasys_channel_mem_bytes_addr_r(cblock_idx, channel_idx),
			perf_pmasys_channel_mem_bytes_addr_ptr_f(addr_lo));
}

int gb10b_perf_init_inst_block(struct gk20a *g, u32 pma_channel_id,
			       u32 inst_blk_ptr, u32 aperture, u32 gfid)
{
	u32 cblock_idx = PMA_CBLOCK_IDX(pma_channel_id);
	struct nvgpu_timeout timeout;
	u32 timeout_ms = 2000U;
	u32 reg_val;

	cblock_idx = PMA_CBLOCK_IDX(pma_channel_id);

	nvgpu_assert(pma_channel_id < g->ops.perf.get_pma_channel_count(g));

	nvgpu_writel(g, perf_pmasys_cblock_bpc_gfid_r(cblock_idx), gfid);
	nvgpu_writel(g, perf_pmasys_cblock_bpc_mem_block_r(cblock_idx), inst_blk_ptr);
	nvgpu_writel(g, perf_pmasys_cblock_bpc_mem_blockupper_r(cblock_idx),
		perf_pmasys_cblock_bpc_mem_blockupper_valid_true_f() |
		nvgpu_aperture_mask_raw(g, aperture,
		     perf_pmasys_cblock_bpc_mem_blockupper_target_sys_ncoh_f(),
		     perf_pmasys_cblock_bpc_mem_blockupper_target_sys_coh_f(),
		     perf_pmasys_cblock_bpc_mem_blockupper_target_lfb_f()));

	nvgpu_timeout_init_cpu_timer_sw(g, &timeout, timeout_ms);
	do {
		reg_val = nvgpu_readl(g, perf_pmasys_cblock_status_r(cblock_idx));
		if ( perf_pmasys_cblock_status_bpc_state_v(reg_val) ==
			perf_pmasys_cblock_status_bpc_state_bound_v()) {
			return 0;
		}
		nvgpu_usleep_range(100, 200);
	} while (nvgpu_timeout_expired(&timeout) == 0);

	nvgpu_err(g, "CBlock BPC didn't bind");
	return -ETIMEDOUT;
}

void gb10b_perf_deinit_inst_block(struct gk20a *g, u32 pma_channel_id)
{
	u32 cblock_idx = PMA_CBLOCK_IDX(pma_channel_id);

	nvgpu_assert(pma_channel_id < g->ops.perf.get_pma_channel_count(g));

	cblock_idx = PMA_CBLOCK_IDX(pma_channel_id);
	nvgpu_writel(g, perf_pmasys_cblock_bpc_gfid_r(cblock_idx), 0U);
	nvgpu_writel(g, perf_pmasys_cblock_bpc_mem_block_r(cblock_idx), 0U);
	nvgpu_writel(g, perf_pmasys_cblock_bpc_mem_blockupper_r(cblock_idx),
		perf_pmasys_cblock_bpc_mem_blockupper_valid_false_f() |
		perf_pmasys_cblock_bpc_mem_blockupper_target_f(0U));
}

int gb10b_perf_update_get_put(struct gk20a *g, u32 pma_channel_id,
			      u64 bytes_consumed, bool update_available_bytes,
			      u64 *put_ptr, bool *overflowed)
{
	u32 cblock_idx = PMA_CBLOCK_IDX(pma_channel_id);
	u32 channel_idx = PMA_CBLOCK_CH_IDX(pma_channel_id);
	u32 val;

	nvgpu_assert(pma_channel_id < g->ops.perf.get_pma_channel_count(g));

	if (bytes_consumed != 0U) {
		nvgpu_writel(g,
			perf_pmasys_channel_mem_bump_r(cblock_idx, channel_idx),
			(u32)bytes_consumed);
	}

	if (update_available_bytes) {
		val = nvgpu_readl(g,
			perf_pmasys_channel_control_user_r(cblock_idx, channel_idx));
		val = set_field(val,
			perf_pmasys_channel_control_user_update_bytes_m(),
			perf_pmasys_channel_control_user_update_bytes_doit_f());
		nvgpu_writel(g,
			perf_pmasys_channel_control_user_r(cblock_idx, channel_idx), val);
	}

	if (put_ptr) {
		*put_ptr = (u64)nvgpu_readl(g,
			perf_pmasys_channel_mem_head_r(cblock_idx, channel_idx));
	}

	if (overflowed) {
		*overflowed = g->ops.perf.get_membuf_overflow_status(g, pma_channel_id);
	}

	return 0;
}

void gb10b_perf_pma_stream_enable(struct gk20a *g, u32 pma_channel_id,
				  bool enable)
{
	u32 cblock_idx = PMA_CBLOCK_IDX(pma_channel_id);
	u32 channel_idx = PMA_CBLOCK_CH_IDX(pma_channel_id);
	u32 reg_val;

	nvgpu_assert(pma_channel_id < g->ops.perf.get_pma_channel_count(g));

	reg_val = nvgpu_readl(g,
			perf_pmasys_channel_config_user_r(cblock_idx, channel_idx));

	if (enable) {
		reg_val = set_field(reg_val,
				perf_pmasys_channel_config_user_stream_m(),
				perf_pmasys_channel_config_user_stream_enable_f());
	} else {
		reg_val = set_field(reg_val,
				perf_pmasys_channel_config_user_stream_m(),
				perf_pmasys_channel_config_user_stream_disable_f());
	}

	nvgpu_writel(g,
		perf_pmasys_channel_config_user_r(cblock_idx, channel_idx), reg_val);
}

int gb10b_perf_wait_for_idle_pma(struct gk20a *g)
{
	struct nvgpu_timeout timeout;
	u32 status;
	u32 timeout_ms = 2000U;
	u32 reg_val;

	nvgpu_timeout_init_cpu_timer_sw(g, &timeout, timeout_ms);

	do {
		reg_val = nvgpu_readl(g, perf_pmasys_enginestatus_r());
		status = perf_pmasys_enginestatus_status_f(reg_val);
		if (status == perf_pmasys_enginestatus_status_empty_v()) {
			return 0;
		}

		nvgpu_usleep_range(20, 40);
	} while (nvgpu_timeout_expired(&timeout) == 0);

	return -ETIMEDOUT;
}

void gb10b_perf_get_num_hwpm_perfmon(struct gk20a *g, u32 *num_sys_perfmon,
				u32 *num_fbp_perfmon, u32 *num_gpc_perfmon)
{
	(void)g;

	/*
	 * Gb10b onwards, as any TPC can be be floorswept with in GPC, perfmons are not
	 * continuous in the GPC range. Hence exact num of perfmon per across GPC chiplets
	 * are not same. So we will capture the existing perfmon masks per each gpc at bind
	 * time based on the floorsweeping mask. These variables are used to store the max
	 * permons per chiplet type.
	 */
	*num_sys_perfmon = perf_pmmsys_engine_sel__size_1_v();
	*num_fbp_perfmon = perf_pmmfbp_engine_sel__size_1_v();
	*num_gpc_perfmon = perf_pmmgpc_engine_sel__size_1_v();
}

static void gb10b_update_gpc_dg_map_status_mask(struct gk20a *g, u32 gr_instance_id,
						u32 num_chiplets, u32 perfmon_idx,
						u32 max_num_perfmons, u32 *dgmap_mask,
						u32 dgmap_mask_size);

void gb10b_perf_disable_all_perfmons(struct gk20a *g, u32 gr_instance_id)
{
	u32 dgmap_status_reg_count = perf_pmmsysrouter_user_dgmap_status_secure__size_1_v();
	u32 num_gpc = nvgpu_grmgr_get_gr_num_gpcs(g, gr_instance_id);
	u32 *gpc_dg_map_mask;
	u32 gpc_dg_map_mask_size;

	g->ops.perf.set_pmm_register(g, perf_pmmsys_control_r(0U), 0U, 1U,
		g->ops.perf.get_pmmsys_per_chiplet_offset(),
		g->num_sys_perfmon);

	g->ops.perf.set_pmm_register(g, perf_pmmfbp_fbps_control_r(0U), 0U, 1U,
		g->ops.perf.get_pmmfbp_per_chiplet_offset(),
		g->num_fbp_perfmon);

	/* Allocate buffers to capture dg map mask for GPC, FBP chiplet */
	gpc_dg_map_mask_size = nvgpu_safe_mult_u32(dgmap_status_reg_count, num_gpc);
	gpc_dg_map_mask = nvgpu_kzalloc(g, sizeof(u32) * gpc_dg_map_mask_size);
	nvgpu_assert(gpc_dg_map_mask != NULL);
	gb10b_update_gpc_dg_map_status_mask(g, gr_instance_id, num_gpc, 0x0U, g->num_gpc_perfmon,
					    gpc_dg_map_mask, gpc_dg_map_mask_size);
	g->ops.perf.set_pmm_register_for_chiplet_range(g, perf_pmmgpc_control_r(0x0U), 0U, 0U,
						       num_gpc,
						       g->ops.perf.get_pmmgpc_per_chiplet_offset(),
						       g->num_gpc_perfmon, gpc_dg_map_mask,
						       gpc_dg_map_mask_size);
	nvgpu_kfree(g, gpc_dg_map_mask);

	if (g->ops.priv_ring.read_pri_fence != NULL) {
		g->ops.priv_ring.read_pri_fence(g);
	}
}

u32 gb10b_perf_get_pmmfbprouter_per_chiplet_offset(void)
{
	/*
	 * No register to find the offset of pmmgpc register.
	 * Difference of pmmgpc register address ranges plus 1 will provide
	 * the offset
	 */
	u32 reg_offset = 1U;

	return (perf_pmmfbprouter_extent_v() - perf_pmmfbprouter_base_v() + reg_offset);
}

u32 gb10b_get_hwpm_fbprouter_perfmon_regs_base(struct gk20a *g)
{
	(void)g;
	return perf_pmmfbprouter_base_v();
}

u32 gb10b_get_hwpm_gpcrouter_perfmon_regs_base(struct gk20a *g)
{
	(void)g;
	return perf_pmmgpcrouter_base_v();
}

u32 gb10b_perf_get_pmmgpcrouter_per_chiplet_offset(void)
{
	/*
	 * No register to find the offset of pmmgpc register.
	 * Difference of pmmgpc register address ranges plus 1 will provide
	 * the offset
	 */
	u32 reg_offset = 1U;

	return (perf_pmmgpcrouter_extent_v() - perf_pmmgpcrouter_base_v() + reg_offset);
}

u32 gb10b_perf_get_pmmsys_per_chiplet_offset(void)
{
	/*
	 * No register to find the offset of pmmsys register.
	 * Difference of pmmsys register address ranges plus 1 will provide
	 * the offset
	 */
	u32 reg_offset = 1U;

	return (perf_pmmsys_extent_v() - perf_pmmsys_base_v() + reg_offset);
}

u32 gb10b_perf_get_pmmfbp_per_chiplet_offset(void)
{
	return (perf_pmmfbp_extent_v() - perf_pmmfbp_base_v() + 0x1U);
}

u32 gb10b_perf_get_pmmgpc_per_chiplet_offset(void)
{
	return (perf_pmmgpc_extent_v() - perf_pmmgpc_base_v() + 0x1U);
}

u32 gb10b_perf_get_gpc_tpc_start_dg_idx(void)
{
	return perf_gpc_streaming_dg_id_gpctpca0_v();
}

u32 gb10b_perf_get_gpc_tpc_end_dg_idx(void)
{
	return perf_gpc_streaming_dg_id_gpctpcd3_v();
}

/*
 * API to check if DG to pma channel mapping is successful.
 */
static int poll_dgmap_status_secure_mapped(struct gk20a *g, u32 reg,
		u32 dgmap_mask_status, bool dg_enable)
{
	struct nvgpu_timeout timeout;
	u32 reg_val;
	bool mapped;
	u32 dg_mapped_mask_bit;
	const u32 timeout_ms = 2000U;
	u32 i = 0U;

	if (nvgpu_platform_is_simulation(g)  && !g->ops.perf.is_perfmon_simulated()) {
		nvgpu_info(g, "Skipping DG mapping poll for simulation");
		return 0;
	}

	dg_mapped_mask_bit = dg_enable ?
		perf_pmmsysrouter_user_dgmap_status_secure_dg0_mapped_v():
		perf_pmmsysrouter_user_dgmap_status_secure_dg0_not_mapped_v();

	nvgpu_timeout_init_cpu_timer_sw(g, &timeout, timeout_ms);

	do {
		reg_val = nvgpu_readl(g, reg);
		mapped = true;
		/*
		 * Index through each bit set in the dgmap_mask and check for dg being
		 * mapped/unmapped based on dg enablement/disablement.
		 */
		for_each_set_bit(i, (const unsigned long *)&dgmap_mask_status,
			PMM_ROUTER_USER_DGMAP_STATUS_SECURE_REG_SIZE_IN_BITS) {
			if ((dg_enable && ((reg_val >> i) & dg_mapped_mask_bit)) ||
				(!dg_enable && (!(((reg_val >> i) & 0x1U) ^ dg_mapped_mask_bit)))) {
				continue;
			}
			mapped = false;
			break;
		}

		if (mapped) {
			return 0;
		}

		nvgpu_usleep_range(20, 40);
	} while (nvgpu_timeout_expired(&timeout) == 0);

	nvgpu_err(g, "DG%d map status not updated, timed out at: 0x%x. With 0x%x", i, reg, reg_val);
	return -ETIMEDOUT;
}

/*
 * API to validate that the pma channel to DG mapping has propogated till its
 * associated router by reading the per chiplet DGMAP_STATUS_SECURE register.
 */
static int poll_dgmap_status_sys_perfmons_mapped(struct gk20a *g,
		u32 *dgmap_mask, u32 dgmap_mask_size, bool dg_enable)
{
	u32 reg;
	u32 i;
	int err = 0;
	u32 dg_map_stride = perf_pmmsysrouter_user_dgmap_status_secure_r(1) -
		perf_pmmsysrouter_user_dgmap_status_secure_r(0);
	u32 dgmap_mask_status;

	nvgpu_assert(perf_pmmsysrouter_user_dgmap_status_secure__size_1_v() - 1U
			<= dgmap_mask_size);
	for (i = 0; i < perf_pmmsysrouter_user_dgmap_status_secure__size_1_v();
			i++) {
		dgmap_mask_status = dgmap_mask[i];
		/*
		 * If any of the dg map status bits are set for that
		 * dgmap_status_secure register(i) poll for MAPPED bit
		 * to be set for those DG's.
		 */
		if (dgmap_mask_status) {
			reg = perf_pmmsysrouter_user_dgmap_status_secure_r(0) +
				(i * dg_map_stride);
			err = poll_dgmap_status_secure_mapped(g, reg,
					dgmap_mask_status, dg_enable);
			if (err != 0) {
				nvgpu_err(g, "poll_dgmap_status_sys_perfmons_mapped failed(%d)",
									err);
				return err;
			}
		}
	}

	return 0;
}

/*
 * API to validate that the pma channel to DG mapping has propogated till its
 * associated router by reading the per chiplet DGMAP_STATUS_SECURE register.
 */
int poll_dgmap_status_gpc_perfmons_mapped(struct gk20a *g, u32 *dgmap_mask,
		u32 dgmap_mask_size, bool dg_enable, u32 cur_gr_instance)
{
	u32 reg;
	u32 i, j;
	int err = 0;
	u32 num_gpc = nvgpu_grmgr_get_gr_num_gpcs(g, cur_gr_instance);
	u32 dg_map_stride = perf_pmmsysrouter_user_dgmap_status_secure_r(1) -
		perf_pmmsysrouter_user_dgmap_status_secure_r(0);
	u32 pmm_gpc_router_chiplet_stride =
		g->ops.perf.get_pmmgpcrouter_per_chiplet_offset();
	u32 dgmap_mask_idx, dgmap_mask_status;
	u32 logical_gpc_id;

	nvgpu_assert(((num_gpc *
		perf_pmmgpcrouter_user_dgmap_status_secure__size_1_v()) - 1U) <=
			dgmap_mask_size);
	for (j = 0; j < num_gpc; j++) {
		logical_gpc_id = nvgpu_grmgr_get_gr_gpc_logical_id(g, cur_gr_instance, j);
		for (i = 0; i < perf_pmmgpcrouter_user_dgmap_status_secure__size_1_v(); i++) {
			dgmap_mask_idx = (j *
					perf_pmmgpcrouter_user_dgmap_status_secure__size_1_v()) + i;
			dgmap_mask_status = dgmap_mask[dgmap_mask_idx];
			/*
			 * If any of the dg map status bits are set for that
			 * dgmap_status_secure register(j) poll for MAPPED bit
			 * to be set for those DG's.
			 */
			if (dgmap_mask_status) {
				reg = perf_pmmgpcrouter_user_dgmap_status_secure_r(0) +
					(i * dg_map_stride) + (logical_gpc_id *
						pmm_gpc_router_chiplet_stride);
				err = poll_dgmap_status_secure_mapped(g, reg,
						dgmap_mask_status, dg_enable);
				if (err != 0) {
					 nvgpu_err(g,
						"poll_dgmap_status_gpc_perfmons_mapped failed(%d)",
						err);
					return err;
				}
			}
		}
	}

	return 0;
}

/*
 * API to validate that the pma channel to DG mapping has propogated till its
 * associated router by reading the per chiplet DGMAP_STATUS_SECURE register.
 */
int poll_dgmap_status_fbp_perfmons_mapped(struct gk20a *g, u32 *dgmap_mask,
					  u32 dgmap_mask_size, bool dg_enable,
					  u32 cur_gr_instance)
{
	u32 reg;
	u32 i, j;
	int err = 0;
	u32 num_fbp = nvgpu_grmgr_get_gr_num_fbps(g, cur_gr_instance);
	u32 dg_map_stride = perf_pmmsysrouter_user_dgmap_status_secure_r(1) -
		perf_pmmsysrouter_user_dgmap_status_secure_r(0);
	u32 pmm_fbp_router_chiplet_stride =
		g->ops.perf.get_pmmfbprouter_per_chiplet_offset();
	u32 dgmap_mask_idx, dgmap_mask_status;
	u32 logical_fbp_id;

	nvgpu_assert(((num_fbp *
		perf_pmmfbprouter_user_dgmap_status_secure__size_1_v()) - 1U) <=
			dgmap_mask_size);
	for (j = 0; j < num_fbp; j++) {
		logical_fbp_id = nvgpu_grmgr_get_fbp_logical_id(g, cur_gr_instance, j);
		for (i = 0; i < perf_pmmfbprouter_user_dgmap_status_secure__size_1_v(); i++) {
			dgmap_mask_idx = (j *
				perf_pmmfbprouter_user_dgmap_status_secure__size_1_v()) + i;
			dgmap_mask_status = dgmap_mask[dgmap_mask_idx];
			/*
			 * If any of the dg map status bits are set for that
			 * dgmap_status_secure register(j), poll for MAPPED
			 * bit to be set for those DG's.
			 */
			if (dgmap_mask_status) {
				reg = perf_pmmfbprouter_user_dgmap_status_secure_r(0) + (i *
					dg_map_stride) + (logical_fbp_id *
						pmm_fbp_router_chiplet_stride);
				err = poll_dgmap_status_secure_mapped(g, reg,
						dgmap_mask_status, dg_enable);
				if (err != 0) {
					nvgpu_err(g,
						"poll_dgmap_status_fbp_perfmons_mapped failed(%d)",
						err);
					return err;
				}
			}
		}
	}

	return 0;
}

static int poll_for_pmm_router_channel_engine_status_empty(struct gk20a *g,
		u32 offset)
{
	struct nvgpu_timeout timeout;
	u32 reg_val;
	u32 status, merged_status;
	u32 timeout_ms = 5000U;

	nvgpu_timeout_init_cpu_timer_sw(g, &timeout, timeout_ms);

	/*
	 * Poll till the current channel data path status and merged data
	 * generators status in router are reported EMPTY. This is to ensure
	 * that the DG is not streaming records.
	 */
	do {
		reg_val = nvgpu_readl(g, offset);
		status =
		perf_pmmsysrouter_user_channel_enginestatus_status_v(reg_val);
		merged_status = perf_pmmsysrouter_user_channel_enginestatus_merged_perfmon_status_v(reg_val);

		if (((status == perf_pmmsysrouter_user_channel_enginestatus_status_empty_v()) ||
			(status == perf_pmmsysrouter_user_channel_enginestatus_status_quiescent_v())) &&
		((merged_status == perf_pmmsysrouter_user_channel_enginestatus_merged_perfmon_status_empty_v()) ||
		 (merged_status == perf_pmmsysrouter_user_channel_enginestatus_merged_perfmon_status_quiescent_v()))) {
			return 0;
		}

		nvgpu_usleep_range(20, 40);
	} while (nvgpu_timeout_expired(&timeout) == 0);

	nvgpu_err(g,
	"PMM router timed out with perfmon status: 0x%x, perfmon offset: 0x%x",
			reg_val, offset);

	return -ETIMEDOUT;
}

/*
 * API to wait for PMM routers in SYS chiplet to report empty
 * status. This is required before we initate DG programming.
 * We have a single SYS chiplet.
 */
static int gb10b_perf_wait_for_idle_pmm_sys_routers(struct gk20a *g, u32 pma_channel_id)
{
	u32 cblock_idx = PMA_CBLOCK_IDX(pma_channel_id);
	u32 channel_idx = PMA_CBLOCK_CH_IDX(pma_channel_id);
	u32 offset = 0U;
	int err;

	/* Wait for channel datapath status in router to report empty */
	offset = perf_pmmsysrouter_user_channel_enginestatus_r(cblock_idx,
			channel_idx);
	err = poll_for_pmm_router_channel_engine_status_empty(g, offset);
	if (err != 0) {
		nvgpu_err(g, "wait_for_idle_pmm_sys_routers failed(%d)", err);
	}

	return err;
}

/*
 * API to wait for PMM routers in each GPC chiplet to report empty
 * status. This is required before we initate DG programming.
 * We have mulitple GPC chiplets.
 */
static int gb10b_perf_wait_for_idle_pmm_gpc_routers(struct gk20a *g, u32 gr_instance_id,
							u32 pma_channel_id)
{
	u32 pmm_gpc_router_chiplet_stride = g->ops.perf.get_pmmgpcrouter_per_chiplet_offset();
	u32 num_gpc, i, logical_gpc_id, offset = 0U;
	int err;
	u32 cblock_idx = PMA_CBLOCK_IDX(pma_channel_id);
	u32 channel_idx = PMA_CBLOCK_CH_IDX(pma_channel_id);

	num_gpc = nvgpu_grmgr_get_gr_num_gpcs(g, gr_instance_id);

	/* Wait for channel datapath status in router to report empty */
	for (i = 0U; i < num_gpc; ++i) {
		logical_gpc_id = nvgpu_grmgr_get_gr_gpc_logical_id(g,
					gr_instance_id, i);
		offset = perf_pmmgpcrouter_user_channel_enginestatus_r(cblock_idx,
				channel_idx) + (logical_gpc_id *
					pmm_gpc_router_chiplet_stride);
		err = poll_for_pmm_router_channel_engine_status_empty(g, offset);
		if (err != 0U) {
			nvgpu_err(g, "wait_for_idle_pmm_gpc_routers failed(%d)", err);
			return err;
		}
	}

	return 0;
}

/*
 * API to wait for PMM routers in each FBP chiplet to report empty
 * status. This is required before we initate DG programming.
 * We have mulitple FBP chiplets.
 */
static int gb10b_perf_wait_for_idle_pmm_fbp_routers(struct gk20a *g, u32 cur_gr_instance,
								u32 pma_channel_id)
{
	u32 pmm_fbp_router_chiplet_stride = g->ops.perf.get_pmmfbprouter_per_chiplet_offset();
	u32 num_fbp = nvgpu_grmgr_get_gr_num_fbps(g, cur_gr_instance);
	u32 i, logical_fbp_id;
	u32 cblock_idx = PMA_CBLOCK_IDX(pma_channel_id);
	u32 channel_idx = PMA_CBLOCK_CH_IDX(pma_channel_id);
	u32 offset = 0U;
	int err;

	/* Wait for channel datapath status in router to report empty */
	for (i = 0U; i < num_fbp; i++) {
		logical_fbp_id = nvgpu_grmgr_get_fbp_logical_id(g,
				cur_gr_instance, i);
		offset = perf_pmmfbprouter_user_channel_enginestatus_r(cblock_idx,
				channel_idx) + (logical_fbp_id *
					pmm_fbp_router_chiplet_stride);
		err = poll_for_pmm_router_channel_engine_status_empty(g, offset);
		if (err != 0U) {
			nvgpu_err(g, "wait_for_idle_pmm_fbp_routers failed(%d)", err);
			return err;
		}
	}

	return 0;
}

/*
 * API to wait for PMM routers in each chiplet(SYS/GPC/FBP) to report
 * empty status. This is required before we initate DG programming.
 * We have single SYS chiplet and mulitple GPC, FBP chiplets.
 */
int gb10b_perf_wait_for_idle_pmm_routers(struct gk20a *g, u32 gr_instance_id, u32 pma_channel_id)
{
	int err;

	err = gb10b_perf_wait_for_idle_pmm_sys_routers(g, pma_channel_id);
	if (err != 0U) {
		return err;
	}

	err = gb10b_perf_wait_for_idle_pmm_gpc_routers(g, gr_instance_id, pma_channel_id);
	if (err != 0U) {
		return err;
	}

	err = gb10b_perf_wait_for_idle_pmm_fbp_routers(g, gr_instance_id, pma_channel_id);
	if (err != 0U) {
		return err;
	}

	return 0;
}

void gb10b_perf_program_pmm_register_for_chiplet_range(struct gk20a *g,
		u32 offset, u32 val, u32 start_chiplet, u32 num_chiplets,
		u32 chiplet_stride, u32 *dg_map_mask)
{
	u32 dgmap_reg_count =
		perf_pmmsysrouter_user_dgmap_status_secure__size_1_v();
	u32 perfmon_index = 0;
	u32 chiplet_index;
	u32 max_chiplet_index = start_chiplet + num_chiplets;
	u32 reg_offset = 0;
	u32 index = 0;

	for (chiplet_index = start_chiplet; chiplet_index < max_chiplet_index; chiplet_index++) {
		for_each_set_bit(perfmon_index, (const unsigned long *)&dg_map_mask[index],
				BITS_PER_BYTE * sizeof(dg_map_mask[index])) {
			reg_offset = offset + (perfmon_index * perf_pmmsys_perdomain_offset_v()) +
				(chiplet_index * chiplet_stride);
			nvgpu_writel(g, reg_offset, val);
		}
		index+= dgmap_reg_count;
	}
}

/*
 * API to update the GPC DGMAP status mask map for the given chiplet and DG index.
 */
static void gb10b_update_gpc_dg_map_status_mask(struct gk20a *g, u32 gr_instance_id,
		u32 num_chiplets, u32 perfmon_idx, u32 max_num_perfmons, u32 *dgmap_mask,
		u32 dgmap_mask_size)
{
	u32 dgmap_reg_count = perf_pmmsysrouter_user_dgmap_status_secure__size_1_v();
	u32 max_perfmon_idx = perfmon_idx + max_num_perfmons;
	u32 tpc_phy_mask, tpc_phy_id;
	u32 gpc_idx, idx;
	u32 dg_reg_idx;
	u32 dg_bit_pos;
	u32 dgmap_mask_idx;
	u32 gpc_phys_id;
	u32 start_tpc_dg_idx = g->ops.perf.get_gpc_tpc_start_dg_idx();
	u32 end_tpc_dg_idx = g->ops.perf.get_gpc_tpc_end_dg_idx();
	u32 num_tpc_per_gpc = nvgpu_get_litter_value(g, GPU_LIT_NUM_TPC_PER_GPC);

	for (gpc_idx = 0U; gpc_idx < num_chiplets; gpc_idx++) {
		gpc_phys_id = nvgpu_grmgr_get_gr_gpc_phys_id(g, gr_instance_id, gpc_idx);
		tpc_phy_mask = g->ops.gr.config.get_gpc_tpc_mask(g, nvgpu_gr_get_config_ptr(g), gpc_phys_id);

		for (idx = perfmon_idx; idx < max_perfmon_idx; idx++) {
			/* skip the floorswept tpc DGs */
			if (idx >= start_tpc_dg_idx && idx <= end_tpc_dg_idx) {
				tpc_phy_id = ((idx - start_tpc_dg_idx) % num_tpc_per_gpc);
				if ((tpc_phy_mask & BIT32(tpc_phy_id)) == 0U) {
					nvgpu_log(g, gpu_dbg_prof,
						"tpc_phy_id %u is floorswept. Skip the DGs for the TPC",
							tpc_phy_id);
					continue;
				}
			}
			dg_reg_idx = idx / 32U;
			dg_bit_pos = idx % 32U;
			dgmap_mask_idx = (gpc_idx * dgmap_reg_count) + dg_reg_idx;
			nvgpu_assert(dgmap_mask_idx < dgmap_mask_size);
			dgmap_mask[dgmap_mask_idx] |= (1UL << dg_bit_pos);
		}
	}
}

void gb10b_update_sys_dg_map_status_mask(struct gk20a *g, u32 gr_instance_id, u32 *sys_dg_map_mask)
{
	u32 gpu_instance_id = nvgpu_grmgr_get_gpu_instance_id(g, gr_instance_id);
	struct nvgpu_gpu_instance *gpu_instance = &g->mig.gpu_instance[gpu_instance_id];
	u32 num_instances = g->mig.num_gpu_instances;
	u32 i;

	/**
	 * In case of non-MIG mode, index is 0x0.
	 *  g->sys_partition_dg_map[][] is bitmap storing DGs available,
	 *  indexed via sys_partition_dg_map[gr_instance_id][DGMAP_STATUS_SECURE]
	 */
	if (num_instances > 0x1U) {
		if (nvgpu_grmgr_get_support_gfx(g, gr_instance_id) == true) {
			/**
			 * gfx capable syspipe is NSR
			 * which means all video engines except OFA.
			 */
			sys_dg_map_mask[0] = NV_PERF_SYS_PARTITION_NSR;
		} else {
			sys_dg_map_mask[0] = NV_PERF_SYS_PARTITION_SR;
		}
	} else {
		/**
		 * Only index 0 needs to be updated because we have only 18 DG's
		 * 18 bits. In scenarios where we have more than 32 DG's,
		 * indices of this array need to be filled with appropriate bits.
		 */
		sys_dg_map_mask[0] = NV_PERF_SYS_PARTITION_ALL_DG;
	}

	if (gpu_instance->num_ofa != 0) {
		sys_dg_map_mask[0] |= BIT32(perf_sys_streaming_dg_id_ofa_v());
	}

	for (i = 0; i < gpu_instance->num_nvdec; i++) {
		sys_dg_map_mask[0] |= BIT32(perf_sys_streaming_dg_id_nvdec0_v() +
						gpu_instance->nvdec_devs[i]->inst_id);
	}
	for (i = 0; i < gpu_instance->num_nvenc; i++) {
		sys_dg_map_mask[0] |= BIT32(perf_sys_streaming_dg_id_nvenc0_v() +
						gpu_instance->nvenc_devs[i]->inst_id);
	}
	for (i = 0; i < gpu_instance->num_nvjpg; i++) {
		sys_dg_map_mask[0] |= BIT32(perf_sys_streaming_dg_id_nvjpg_v() +
						gpu_instance->nvjpg_devs[i]->inst_id);
	}
}

/*
 * API to update the DGMAP status mask map for the FBP chiplet and DG index.
 */
void gb10b_update_fbp_dg_map_status_mask(u32 num_chiplets, u32 perfmon_idx,
			u32 num_perfmons, u32 *dgmap_mask, u32 dgmap_mask_size)
{
	u32 chiplet_idx, idx;
	u32 dg_reg_idx;
	u32 dg_bit_pos;
	u32 dgmap_mask_idx;
	u32 max_perfmon_idx = perfmon_idx + num_perfmons;
	u32 dgmap_reg_count =
		perf_pmmsysrouter_user_dgmap_status_secure__size_1_v();

	for (chiplet_idx = 0U; chiplet_idx < num_chiplets; chiplet_idx++) {
		for (idx = perfmon_idx; idx < max_perfmon_idx; idx++) {
			/*
			 * No floor sweeping needs to be considered for LTS.
			 * as all 4 LTS per LTC will be active always.
			 */
			dg_reg_idx = idx / 32U;
			dg_bit_pos = idx % 32U;
			dgmap_mask_idx = (chiplet_idx * dgmap_reg_count) +
				dg_reg_idx;
			nvgpu_assert(dgmap_mask_idx < dgmap_mask_size);
			dgmap_mask[dgmap_mask_idx] |= (1UL << dg_bit_pos);
		}
	}
}

/*
 * API to update the DGMAP status mask map for the given GPC, TPC and CAU DG index.
 */
void gb10b_update_cau_dg_map_status_mask(struct gk20a *g, u32 gr_instance_id, u32 num_gpcs,
					 u32 perfmon_idx, u32 num_perfmons, u32 *dgmap_mask,
					 u32 dgmap_mask_size)
{
	struct nvgpu_gr_config *gr_config = nvgpu_gr_get_config_ptr(g);
	u32 gpc_idx, tpc_idx, idx;
	u32 gpc_phy_idx, tpc_phy_idx;
	u32 dg_reg_idx;
	u32 dg_idx;
	u32 dg_bit_pos;
	u32 dgmap_mask_idx;
	u32 num_tpc = 0U;
	u32 max_perfmon_idx = nvgpu_safe_add_u32(perfmon_idx, num_perfmons);
	u32 dgmap_reg_count = perf_pmmsysrouter_user_dgmap_status_secure__size_1_v();

	for (gpc_idx = 0U; gpc_idx < num_gpcs; gpc_idx++) {

		/*
		 * TODO: NVGPU-10557 Replace the below hal usage with
		 * nvgpu_gr_config_get_gpc_tpc_count once the sw struct is correctly
		 * updated with tpc count per gpc.
		 */
		gpc_phy_idx = nvgpu_grmgr_get_gr_gpc_phys_id(g, gr_instance_id,
								gpc_idx);
		num_tpc = g->ops.gr.config.get_tpc_count_in_gpc(g, nvgpu_gr_get_config_ptr(g),
				nvgpu_grmgr_get_gr_gpc_logical_id(g, gr_instance_id,
					gpc_idx));

		for (tpc_idx = 0U; tpc_idx < num_tpc; tpc_idx++) {
			tpc_phy_idx = nvgpu_gr_config_get_gpc_tpc_phy_id(gr_config,
							gpc_phy_idx, tpc_idx);
			for (idx = perfmon_idx; idx < max_perfmon_idx; idx++) {
				dg_idx = nvgpu_safe_add_u32(idx, tpc_phy_idx);
				dg_reg_idx = dg_idx / 32U;
				dg_bit_pos = (dg_idx % 32U);
				dgmap_mask_idx = nvgpu_safe_add_u32(nvgpu_safe_mult_u32(gpc_idx,
										dgmap_reg_count),
								    dg_reg_idx);
				nvgpu_assert(dgmap_mask_idx < dgmap_mask_size);
				dgmap_mask[dgmap_mask_idx] |= (1UL << dg_bit_pos);
				nvgpu_log(g, gpu_dbg_prof,
					  "tpc_idx: %d: tpc_phy_idx: %d, dgmap_mask[%d]: 0x%x",
					  tpc_idx, tpc_phy_idx, dgmap_mask_idx,
					  dgmap_mask[dgmap_mask_idx]);
			}
		}
	}
}

void
gb10b_perf_set_pmm_register_for_chiplet_range(struct gk20a *g, u32 offset, u32 val,
					      u32 start_chiplet, u32 num_chiplets,
					      u32 chiplet_stride, u32 num_perfmons,
					      u32 *dg_map_mask, u32 dg_map_mask_size)
{
	u32 dg_map_reg_count = perf_pmmsysrouter_user_dgmap_status_secure__size_1_v();
	u32 dg_reg_idx, dg_bit_pos, dg_map_mask_idx;
	u32 perfmon_index;
	u32 chiplet_index;
	u32 max_chiplet_index = nvgpu_safe_add_u32(start_chiplet, num_chiplets);
	u32 reg_offset;

	for (chiplet_index = start_chiplet; chiplet_index < max_chiplet_index; chiplet_index++) {
		for (perfmon_index = 0U; perfmon_index < num_perfmons; perfmon_index++) {
			if (dg_map_mask != NULL) {
				dg_reg_idx = perfmon_index / 32U;
				dg_bit_pos = perfmon_index % 32U;
				dg_map_mask_idx =
					nvgpu_safe_add_u32(nvgpu_safe_mult_u32(chiplet_index,
									       dg_map_reg_count),
							   dg_reg_idx);
				nvgpu_assert(dg_map_mask_idx < dg_map_mask_size);

				/*
				 * if the perfmon don't exist due to floorsweeping skip the
				 * programming.
				 */
				if ((dg_map_mask[dg_map_mask_idx] & (1UL << dg_bit_pos)) == 0U) {
					continue;
				}
			}

			reg_offset = nvgpu_safe_mult_u32(perfmon_index,
							 perf_pmmsys_perdomain_offset_v());
			reg_offset = nvgpu_safe_add_u32(reg_offset,
							nvgpu_safe_mult_u32(chiplet_index,
									    chiplet_stride));
			reg_offset = nvgpu_safe_add_u32(reg_offset, offset);
			nvgpu_writel(g, reg_offset, val);
		}
	}
}

/*
 * API to program secure config register for HWPM Data Generators(perfmons).
 * DG's like CAU, CCU and HEM are not programmed as part of this api
 * There are 3 parts to it:
 * 1. Wait for PMM Chiplet routers to report idle, which implies DG's are
 *    disabled and routers are reporting EMPTY status.
 * 2. Program secure config to enable config record generation.
 * 3. Validate that the pma channel to DG mapping is reflected in router by
 *    reading status registers.
 */
int gb10b_set_secure_config_for_hwpm_dg(struct gk20a *g, u32 gr_instance_id,
					u32 pma_channel_id, bool dg_enable)
{
	u32 cblock_idx = PMA_CBLOCK_IDX(pma_channel_id);
	u32 channel_idx = PMA_CBLOCK_CH_IDX(pma_channel_id);
	int err;
	u32 *sys_dg_map_mask;
	u32 sys_dg_map_mask_size;
	u32 *gpc_dg_map_mask;
	u32 gpc_dg_map_mask_size;
	u32 *fbp_dg_map_mask;
	u32 fbp_dg_map_mask_size;
	u32 start_gpc;
	u32 start_fbp;
	u32 config_val = 0U;
	u32 i;
	u32 num_gpc = nvgpu_grmgr_get_gr_num_gpcs(g, gr_instance_id);
	u32 num_fbp = nvgpu_grmgr_get_gr_num_fbps(g, gr_instance_id);
	u32 dgmap_status_reg_count =
		perf_pmmsysrouter_user_dgmap_status_secure__size_1_v();

	/* Get logical GPC ID for the 1st local GPC in that MIG instance. */
	start_gpc = nvgpu_grmgr_get_gr_gpc_logical_id(g, gr_instance_id, 0x0U);
	/* Get logical FBP index for the 1st local FBP chiplet in that MIG instance. */
	start_fbp = nvgpu_grmgr_get_fbp_logical_id(g, gr_instance_id, 0x0U);

	nvgpu_log(g, gpu_dbg_prof,
		"num_gpc: %u, start_gpc: %u, num_fbp: %u, start_fbp: %u",
		num_gpc, start_gpc, num_fbp, start_fbp);

	/* Wait for Pmm routers to be idle before we program secure config for DGs. */
	err = g->ops.perf.wait_for_idle_pmm_routers(g, gr_instance_id, pma_channel_id);
	if (err != 0) {
		return err;
	}

	sys_dg_map_mask = &(g->sys_partition_dg_map[gr_instance_id][0]);
	sys_dg_map_mask_size = dgmap_status_reg_count * 0x1U;

	/* Allocate buffers to capture dg map mask for GPC, FBP chiplet */
	gpc_dg_map_mask_size = dgmap_status_reg_count * num_gpc;
	gpc_dg_map_mask = nvgpu_kzalloc(g, sizeof(u32) * gpc_dg_map_mask_size);
	if (gpc_dg_map_mask == NULL) {
		err = -ENOMEM;
		goto label_gpc_dg_map_mask;
	}

	fbp_dg_map_mask_size = dgmap_status_reg_count * num_fbp;
	fbp_dg_map_mask = nvgpu_kzalloc(g, sizeof(u32) * fbp_dg_map_mask_size);
	if (fbp_dg_map_mask == NULL) {
		err = -ENOMEM;
		goto label_fbp_dg_map_mask;
	}

	if (dg_enable) {
		config_val = set_field(config_val,
				perf_pmmsys_secure_config_cblock_id_m(), perf_pmmsys_secure_config_cblock_id_f(cblock_idx));
		config_val = set_field(config_val,
				perf_pmmsys_secure_config_channel_id_m(),
				perf_pmmsys_secure_config_channel_id_f(channel_idx));
		config_val = set_field(config_val, perf_pmmsys_secure_config_mapped_m(),
				perf_pmmsys_secure_config_mapped_true_f());
		config_val = set_field(config_val,
				perf_pmmsys_secure_config_cmd_slice_id_m(), perf_pmmsys_secure_config_cmd_slice_id_f(pma_channel_id));
		config_val = set_field(config_val,
				perf_pmmsys_secure_config_command_pkt_decoder_m(),
				perf_pmmsys_secure_config_command_pkt_decoder_enable_f());
		config_val = set_field(config_val,
				perf_pmmsys_secure_config_use_prog_dg_idx_m(),
				perf_pmmsys_secure_config_use_prog_dg_idx_false_f());
		config_val = set_field(config_val,
				perf_pmmsys_secure_config_dg_idx_m(),
				perf_pmmsys_secure_config_dg_idx_f(0U));
		config_val = set_field(config_val,
				perf_pmmsys_secure_config_ignore_cmd_pkt_reset_m(),
				perf_pmmsys_secure_config_ignore_cmd_pkt_reset_false_f());

	} else {
		config_val = set_field(config_val, perf_pmmsys_secure_config_mapped_m(),
				perf_pmmsys_secure_config_mapped_false_f());
	}


	/* SYS */
	g->ops.perf.update_sys_dg_map_status_mask(g, gr_instance_id, sys_dg_map_mask);
	nvgpu_log(g, gpu_dbg_prof, "gr_instance_id: %u, sys_dg_map_mask[0]: 0x%X",
						gr_instance_id, sys_dg_map_mask[0]);
	g->ops.perf.program_sys_pmm_secure_config(g,
			perf_pmmsys_secure_config_r(0U), config_val,
			sys_dg_map_mask[0]);

	/* FBP */
	gb10b_update_fbp_dg_map_status_mask(num_fbp, 0x0U, g->num_fbp_perfmon,
			fbp_dg_map_mask, fbp_dg_map_mask_size);
	gb10b_perf_program_pmm_register_for_chiplet_range(g,
			perf_pmmfbp_secure_config_r(0U),
			config_val, start_fbp, num_fbp,
			g->ops.perf.get_pmmfbp_per_chiplet_offset(),
			fbp_dg_map_mask);

	for (i = 0; i < num_fbp; i++) {
		nvgpu_log(g, gpu_dbg_prof, "gr_instance_id: %u, fbp_dg_map_mask[%u]: 0x%X",
			gr_instance_id, i * dgmap_status_reg_count,
			fbp_dg_map_mask[i * dgmap_status_reg_count]);
	}

	/* GPC */
	gb10b_update_gpc_dg_map_status_mask(g, gr_instance_id, num_gpc, 0x0U, g->num_gpc_perfmon,
			gpc_dg_map_mask, gpc_dg_map_mask_size);
	gb10b_perf_program_pmm_register_for_chiplet_range(g,
			perf_pmmgpc_secure_config_r(0U),
			config_val, start_gpc, num_gpc,
			g->ops.perf.get_pmmgpc_per_chiplet_offset(),
			gpc_dg_map_mask);

	for (i = 0; i < num_gpc; i++) {
		nvgpu_log(g, gpu_dbg_prof, "gr_instance_id: %u, gpc_dg_map_mask[%u]: 0x%X",
				gr_instance_id, i * dgmap_status_reg_count,
				gpc_dg_map_mask[i * dgmap_status_reg_count]);
	}

	/* Makes sure writes to all the chiplets are completed. */
	if (g->ops.priv_ring.read_pri_fence != NULL) {
		g->ops.priv_ring.read_pri_fence(g);
	}

	/* SYS */
	err = poll_dgmap_status_sys_perfmons_mapped(g, sys_dg_map_mask,
			sys_dg_map_mask_size, dg_enable);
	if (err != 0) {
		goto label_fail;
	}

	/* GPC */
	err = poll_dgmap_status_gpc_perfmons_mapped(g, gpc_dg_map_mask,
			gpc_dg_map_mask_size, dg_enable, gr_instance_id);
	if (err != 0) {
		goto label_fail;
	}

	/* FBP */
	err = poll_dgmap_status_fbp_perfmons_mapped(g, fbp_dg_map_mask,
			fbp_dg_map_mask_size, dg_enable, gr_instance_id);

label_fail:
	nvgpu_kfree(g, fbp_dg_map_mask);
label_fbp_dg_map_mask:
	nvgpu_kfree(g, gpc_dg_map_mask);
label_gpc_dg_map_mask:
	return err;
}

/* CWD HEM API's */
const u32 cwd_hem_regs[] =
{
	/* This list is autogenerated. Do not edit. */
	0x002b6c00,
	0x002b6c08,
	0x002b6c0c,
	0x002b6c10,
	0x002b6c20,
	0x002b6c24,
	0x002b6c38,
	0x002b6c28,
	0x002b6c34,
	0x002b6c3c,
};

static const u32 *gb10b_perf_get_cwd_hem_regs(u32 *count)
{
	*count = sizeof(cwd_hem_regs) / sizeof(cwd_hem_regs[0]);
	return cwd_hem_regs;
}

u32 gb10b_perf_get_num_cwd_hems(void)
{
	return perf_hem_sys0_secure_config__size_1_v();
}

static void gb10b_perf_set_cwd_hem_registers(struct gk20a *g, u32 offset, u32 val,
				u32 start_hem_idx, u32 num_hems)
{
	u32 hem_index;
	u32 reg_offset = 0;
	u32 last_hem_index = start_hem_idx + num_hems;

	for (hem_index = start_hem_idx; hem_index < last_hem_index; hem_index++) {
		reg_offset = offset + hem_index * perf_hem_sys0_perhem_offset_v();
		nvgpu_writel(g, reg_offset, val);
	}
}

static u32 gb10b_get_tpc_logical_id(struct gk20a *g, u32 tpc_cnt, u32 gpc_phy_id, u32 tpc_phy_idx)
{
	u32 tpc_log_idx;

	for (tpc_log_idx = 0; tpc_log_idx < tpc_cnt; tpc_log_idx++) {
		if (tpc_phy_idx ==
			nvgpu_gr_config_get_gpc_tpc_phy_id(nvgpu_gr_get_config_ptr(g),
								gpc_phy_id, tpc_log_idx)) {
			return tpc_log_idx;
		}
	}
	nvgpu_err(g, "Failed: tpc_phy_id %u gpc_phy_id %u tpc_cnt %u",
		 tpc_phy_idx, gpc_phy_id, tpc_cnt);
	nvgpu_assert(0);

	return U32_MAX;
}

static void
gb10b_perf_set_cau_registers(struct gk20a *g, u32 gr_instance_id, u32 offset, u32 val,
				u32 start_gpc, u32 num_gpc, u32 gpc_stride,
				u32 tpc_stride, u32 *dg_map_mask)
{
	u32 dgmap_reg_count =
		perf_pmmsysrouter_user_dgmap_status_secure__size_1_v();
	u32 max_gpc_index = start_gpc + num_gpc;
	u32 dgmap_reg_idx, dg_idx;
	u32 gpc_log_idx, gpc_phy_idx;
	u32 tpc_log_idx, tpc_phy_idx;
	u32 reg_offset, num_tpc;

	for (gpc_log_idx = start_gpc; gpc_log_idx < max_gpc_index; gpc_log_idx++) {
		gpc_phy_idx = nvgpu_grmgr_get_gr_gpc_phys_id(g, gr_instance_id,
								gpc_log_idx - start_gpc);
		/*
		 * TODO: NVGPU-10557 Replace the below hal usage with
		 * nvgpu_gr_config_get_gpc_tpc_count once the sw struct is correctly
		 * updated with tpc count per gpc.
		 */
		num_tpc = g->ops.gr.config.get_tpc_count_in_gpc(g, nvgpu_gr_get_config_ptr(g),
				(gpc_log_idx - start_gpc));
		nvgpu_log(g, gpu_dbg_prof, "logical gpc: %d, num_tpc: %d", gpc_log_idx, num_tpc);

		dgmap_reg_idx = (gpc_log_idx - start_gpc) * dgmap_reg_count;
		for_each_set_bit(dg_idx, (const unsigned long *)&dg_map_mask[dgmap_reg_idx],
				BITS_PER_BYTE * sizeof(dg_map_mask[dgmap_reg_idx])) {
			/*
			 * In streaming DG ID for tpc cau DGs, tpc index are physical But the tpc
			 * strides are based on tpc logical index. Hence we check which physical
			 * TPC's DG is present and then we program into a register based on
			 * corresponding logical tpc index.
			 */
			tpc_phy_idx = dg_idx - g->ops.perf.get_gpc_tpc0_cau0_dg_idx();
			tpc_log_idx = gb10b_get_tpc_logical_id(g, num_tpc, gpc_phy_idx, tpc_phy_idx);
			reg_offset = offset + ((gpc_log_idx -start_gpc) * gpc_stride) +
							(tpc_log_idx * tpc_stride);
			nvgpu_log(g, gpu_dbg_prof, "dg_idx: %u, reg_offset: 0x%08x: val: 0x%08x",
					dg_idx, reg_offset, val);
			nvgpu_writel(g, reg_offset, val);
		}
	}
}

void gb10b_perf_reset_cwd_hem_registers(struct gk20a *g, u32 gr_instance_id)
{
	const u32 *regs;
	u32 i, count;
	u32 start_hem_idx = 0U;
	u32 num_gpu_instances = g->mig.num_gpu_instances;
	u32 num_cwd_hems = g->ops.perf.get_num_cwd_hems();

	regs = gb10b_perf_get_cwd_hem_regs(&count);
	/**
	 * In case of MIG, the number of CWD HEMS are equally distributed.
	 * We have only 2 CWD HEMS.
	 * In case of native config, all the CWD HEMS are visible to user.
	 */
	num_cwd_hems = num_cwd_hems / num_gpu_instances;
	if (num_gpu_instances > 1U) {
		if (nvgpu_grmgr_get_support_gfx(g, gr_instance_id) == true) {
			/**
			 * Gfx capable syspipe will be NSR.
			 * GR engine 0, SYSPIPE 0  will be assigned to NSR.
			 */
			start_hem_idx = 0U;
		} else {
			start_hem_idx = 1U;
		}
	}

	for (i = 0U; i < count; i++) {
		gb10b_perf_set_cwd_hem_registers(g, regs[i], 0U, start_hem_idx,
				num_cwd_hems);
	}

	if (g->ops.priv_ring.read_pri_fence != NULL) {
		g->ops.priv_ring.read_pri_fence(g);
	}
}

u32 gb10b_perf_get_gpc_tpc0_cau0_dg_idx(void)
{
	return perf_gpc_streaming_dg_id_tpc0cau0_v();
}

int gb10b_perf_set_secure_config_for_cau(struct gk20a *g, u32 gr_instance_id,
					 u32 pma_channel_id, bool dg_enable)
{
	u32 dgmap_status_reg_count = perf_pmmsysrouter_user_dgmap_status_secure__size_1_v();
	u32 cblock_idx = PMA_CBLOCK_IDX(pma_channel_id);
	u32 channel_idx = PMA_CBLOCK_CH_IDX(pma_channel_id);
	u32 *gpc_dg_map_mask;
	u32 gpc_dg_map_mask_size;
	u32 config_val = 0;
	u32 num_gpc, start_gpc;
	u32 cau_dg_idx;
	int err;
	u32 i;

	num_gpc = nvgpu_grmgr_get_gr_num_gpcs(g, gr_instance_id);

	/* Get logical GPC ID for the 1st local GPC in that MIG instance. */
	start_gpc = nvgpu_grmgr_get_gr_gpc_logical_id(g, gr_instance_id, 0x0U);

	nvgpu_log(g, gpu_dbg_prof, "num_gpc: %u, start_gpc: %u", num_gpc, start_gpc);
	err = gb10b_perf_wait_for_idle_pmm_gpc_routers(g, gr_instance_id, pma_channel_id);
	if (err != 0) {
		return err;
	}

	gpc_dg_map_mask_size = dgmap_status_reg_count * num_gpc;
	gpc_dg_map_mask = nvgpu_kzalloc(g, sizeof(u32) * gpc_dg_map_mask_size);
	if (gpc_dg_map_mask == NULL) {
		return -ENOMEM;
	}

	if (dg_enable) {
		config_val = set_field(config_val, gr_gpc0_tpc0_cau_secure_config_cblock_id_m(),
				gr_gpc0_tpc0_cau_secure_config_cblock_id_f(cblock_idx));
		config_val = set_field(config_val, gr_gpc0_tpc0_cau_secure_config_channel_id_m(),
				gr_gpc0_tpc0_cau_secure_config_channel_id_f(channel_idx));
		config_val = set_field(config_val, gr_gpc0_tpc0_cau_secure_config_cmd_slice_id_m(),
				gr_gpc0_tpc0_cau_secure_config_cmd_slice_id_f(pma_channel_id));
		config_val = set_field(config_val, gr_gpc0_tpc0_cau_secure_config_mapped_m(),
				gr_gpc0_tpc0_cau_secure_config_mapped_true_f());
		config_val = set_field(config_val,
				gr_gpc0_tpc0_cau_secure_config_command_pkt_decoder_m(),
				gr_gpc0_tpc0_cau_secure_config_command_pkt_decoder_enable_f());
		config_val = set_field(config_val,
				gr_gpc0_tpc0_cau_secure_config_dg_idx_m(), gr_gpc0_tpc0_cau_secure_config_dg_idx_f(0U));
		config_val = set_field(config_val,
				gr_gpc0_tpc0_cau_secure_config_use_prog_dg_id_m(),
				gr_gpc0_tpc0_cau_secure_config_use_prog_dg_id_false_f());
		config_val = set_field(config_val,
				gr_gpc0_tpc0_cau_secure_config_ignore_cmd_pkt_reset_m(),
				gr_gpc0_tpc0_cau_secure_config_ignore_cmd_pkt_reset_false_f());

	} else {
		config_val = set_field(config_val, gr_gpc0_tpc0_cau_secure_config_mapped_m(),
				gr_gpc0_tpc0_cau_secure_config_mapped_false_f());
	}

	/* GPC */
	cau_dg_idx = g->ops.perf.get_gpc_tpc0_cau0_dg_idx();
	g->ops.perf.update_cau_dg_map_status_mask(g, gr_instance_id, num_gpc, cau_dg_idx,
			gr_gpc0_tpc0_cau_secure_config__size_1_v(), gpc_dg_map_mask,
			gpc_dg_map_mask_size);
	gb10b_perf_set_cau_registers(g, gr_instance_id, gr_gpc0_tpc0_cau_secure_config_r(0U),
			config_val, start_gpc, num_gpc, proj_gpc_stride_v(),
			proj_tpc_in_gpc_stride_v(), gpc_dg_map_mask);

	for (i = 0; i < num_gpc; i++) {
		nvgpu_log(g, gpu_dbg_prof,
				"gr_instance_id: %u, gpc_dg_map_mask[%u]: 0x%X",
				gr_instance_id, i * dgmap_status_reg_count,
				gpc_dg_map_mask[i * dgmap_status_reg_count]);
	}

	/* Makes sure writes to all the chiplets are completed. */
	if (g->ops.priv_ring.read_pri_fence != NULL) {
		g->ops.priv_ring.read_pri_fence(g);
	}

	/* GPC */
	err = poll_dgmap_status_gpc_perfmons_mapped(g, gpc_dg_map_mask, gpc_dg_map_mask_size,
			dg_enable, gr_instance_id);

	nvgpu_kfree(g, gpc_dg_map_mask);

	return err;
}

void gb10b_perf_update_sys_hem_cwd_dg_map_mask(struct gk20a *g, u32 gr_instance_id,
					      u32 *sys_dg_map_mask, u32 *start_hem_idx,
					      u32 *num_hems)
{
	/* g->mig.num_gpu_instances includes physical gpu instance +  "N" mig instances */
	u32 num_mig_instances = g->mig.num_gpu_instances - 0x1U;

	if (g->mig.num_gpu_instances > 0x1U) {
		if (nvgpu_grmgr_get_support_gfx(g, gr_instance_id) == true) {
			*start_hem_idx = 0x0U;
			sys_dg_map_mask[0] = NV_PERF_SYS_PARTITION_HEMCWD0;
		} else {
			*start_hem_idx = 0x1U;
			sys_dg_map_mask[0] = NV_PERF_SYS_PARTITION_HEMCWD1;
		}
		/* HEM's are equally distributed across MIG instances */
		*num_hems = g->ops.perf.get_num_cwd_hems() / num_mig_instances;
	} else {
		sys_dg_map_mask[0] = NV_PERF_SYS_PARTITION_ALL_HEMS;
		*num_hems = g->ops.perf.get_num_cwd_hems();
		*start_hem_idx = 0U;
	}
}

int gb10b_perf_set_secure_config_for_cwd_hem(struct gk20a *g, u32 gr_instance_id,
					     u32 pma_channel_id, bool dg_enable)
{
	u32 dgmap_status_reg_count = perf_pmmsysrouter_user_dgmap_status_secure__size_1_v();
	u32 cblock_idx = PMA_CBLOCK_IDX(pma_channel_id);
	u32 channel_idx = PMA_CBLOCK_CH_IDX(pma_channel_id);
	u32 start_hem_idx, num_hems;
	u32 *sys_dg_map_mask;
	u32 sys_dg_map_mask_size;
	u32 config_val = 0;
	int err;

	err = gb10b_perf_wait_for_idle_pmm_sys_routers(g, pma_channel_id);
	if (err != 0) {
		return err;
	}

	sys_dg_map_mask_size = dgmap_status_reg_count * 0x1U;
	sys_dg_map_mask = &(g->sys_partition_dg_map[gr_instance_id][0]);
	(void) memset(sys_dg_map_mask, 0x0U, sizeof(*sys_dg_map_mask));

	if (dg_enable) {
		config_val = set_field(config_val,
				perf_hem_sys0_secure_config_cblock_id_m(),
				perf_hem_sys0_secure_config_cblock_id_f(cblock_idx));
		config_val = set_field(config_val,
				perf_hem_sys0_secure_config_channel_id_m(),
				perf_hem_sys0_secure_config_channel_id_f(channel_idx));
		config_val = set_field(config_val,
				perf_hem_sys0_secure_config_cmd_slice_id_m(),
				pma_channel_id);
		config_val = set_field(config_val, perf_hem_sys0_secure_config_mapped_m(),
				perf_hem_sys0_secure_config_mapped_true_f());
		config_val = set_field(config_val,
				perf_hem_sys0_secure_config_command_pkt_decoder_m(),
				perf_hem_sys0_secure_config_command_pkt_decoder_enable_f());
		config_val = set_field(config_val,
				perf_hem_sys0_secure_config_use_prog_dg_idx_m(),
				perf_hem_sys0_secure_config_use_prog_dg_idx_false_f());
		config_val = set_field(config_val,
				perf_hem_sys0_secure_config_dg_idx_m(), perf_hem_sys0_secure_config_dg_idx_f(0U));
		config_val = set_field(config_val,
				perf_hem_sys0_secure_config_ignore_cmd_pkt_reset_m(),
				perf_hem_sys0_secure_config_ignore_cmd_pkt_reset_false_f());
	} else {
		config_val = set_field(config_val,
				perf_hem_sys0_secure_config_mapped_m(),
				perf_hem_sys0_secure_config_mapped_false_f());
	}

	g->ops.perf.update_sys_hem_cwd_dg_map_mask(g, gr_instance_id, sys_dg_map_mask,
						  &start_hem_idx, &num_hems);

	nvgpu_log(g, gpu_dbg_prof,
		  "gr_instance_id: %u sys_dg_map_mask[0]: 0x%X, num_hems: %u, start_hem_idx: %u",
		  gr_instance_id, sys_dg_map_mask[0], num_hems, start_hem_idx);

	/* CWD unit resides only in SYS chiplet. */
	gb10b_perf_set_cwd_hem_registers(g, perf_hem_sys0_secure_config_r(0),
			config_val, start_hem_idx, num_hems);

	/* Makes sure writes to all the chiplets are completed. */
	if (g->ops.priv_ring.read_pri_fence != NULL) {
		g->ops.priv_ring.read_pri_fence(g);
	}

	err = poll_dgmap_status_sys_perfmons_mapped(g, sys_dg_map_mask,
			sys_dg_map_mask_size, dg_enable);

	return err;
}

void gb10b_perf_init_hwpm_pmm_register(struct gk20a *g, u32 gr_instance_id)
{
	u32 dgmap_status_reg_count = perf_pmmsysrouter_user_dgmap_status_secure__size_1_v();
	u32 num_gpc = nvgpu_grmgr_get_gr_num_gpcs(g, gr_instance_id);
	u32 num_fbp = nvgpu_grmgr_get_gr_num_fbps(g, gr_instance_id);
	u32 start_gpc = nvgpu_grmgr_get_gr_gpc_logical_id(g, gr_instance_id, 0x0U);
	u32 start_fbp = nvgpu_grmgr_get_fbp_logical_id(g, gr_instance_id, 0x0U);
	u32 *gpc_dg_map_mask;
	u32 gpc_dg_map_mask_size;

	g->ops.perf.set_pmm_register(g, perf_pmmsys_engine_sel_r(0x0U),
				   U32_MAX, 0x1U,
				   g->ops.perf.get_pmmsys_per_chiplet_offset(),
				   g->num_sys_perfmon);
	g->ops.perf.set_pmm_register_for_chiplet_range(g,
			perf_pmmfbp_engine_sel_r(0x0U),
			U32_MAX, start_fbp, num_fbp,
			g->ops.perf.get_pmmfbp_per_chiplet_offset(),
			g->num_fbp_perfmon, NULL, 0);

	/* Allocate buffers to capture dg map mask for GPC, FBP chiplet */
	gpc_dg_map_mask_size = nvgpu_safe_mult_u32(dgmap_status_reg_count, num_gpc);
	gpc_dg_map_mask = nvgpu_kzalloc(g, sizeof(u32) * gpc_dg_map_mask_size);
	nvgpu_assert(gpc_dg_map_mask != NULL);
	gb10b_update_gpc_dg_map_status_mask(g, gr_instance_id, num_gpc, 0x0U, g->num_gpc_perfmon,
					    gpc_dg_map_mask, gpc_dg_map_mask_size);

	g->ops.perf.set_pmm_register_for_chiplet_range(g,
			perf_pmmgpc_engine_sel_r(0x0U),
			U32_MAX, start_gpc, num_gpc,
			g->ops.perf.get_pmmgpc_per_chiplet_offset(),
			g->num_gpc_perfmon, gpc_dg_map_mask, gpc_dg_map_mask_size);
	nvgpu_kfree(g, gpc_dg_map_mask);

	if (g->ops.priv_ring.read_pri_fence != NULL) {
		/* Read back to ensure all writes are complete */
		g->ops.priv_ring.read_pri_fence(g);
	}
}

void gb10b_perf_reset_pm_trigger_masks(struct gk20a *g, u32 pma_channel_id,
			u32 gr_instance_id, u32 reservation_id)
{
	(void) gr_instance_id;
	(void) reservation_id;

	nvgpu_writel(g, perf_pmasys_command_slice_trigger_mask_secure0_r(pma_channel_id), 0U);
}

static void gb10b_perf_enable_pm_hes_trigger(struct gk20a *g, u32 gr_instance_id,
					     u32 pma_channel_id, bool event)
{
	u32 reg_val = nvgpu_readl(g,
			perf_pmasys_command_slice_trigger_mask_secure0_r(pma_channel_id));
	u32 syspipe_id;

	if (nvgpu_grmgr_is_multi_gr_enabled(g)) {
		syspipe_id = nvgpu_grmgr_get_gr_syspipe_id(g, gr_instance_id);
		if (syspipe_id == 0)
			reg_val |= (event ? BIT(perf_pmasys_engine_index_grevents_v()) :
					    BIT(perf_pmasys_engine_index_gr_v()));
		else
			reg_val |= (event ? BIT(perf_pmasys_engine_index_grevents1_v()) :
					    BIT(perf_pmasys_engine_index_gr1_v()));
	} else {
		reg_val |= (event ? (BIT(perf_pmasys_engine_index_grevents_v()) |
					BIT(perf_pmasys_engine_index_grevents1_v())) :
					(BIT(perf_pmasys_engine_index_gr_v()) |
					BIT(perf_pmasys_engine_index_gr1_v())));
	}
	nvgpu_writel(g, perf_pmasys_command_slice_trigger_mask_secure0_r(pma_channel_id), reg_val);
}

void gb10b_perf_enable_pm_trigger(struct gk20a *g, u32 gr_instance_id, u32 pma_channel_id,
									u32 reservation_id)
{
	(void) reservation_id;

	gb10b_perf_enable_pm_hes_trigger(g, gr_instance_id, pma_channel_id, false);
}

void gb10b_perf_enable_hes_event_trigger(struct gk20a *g, u32 gr_instance_id, u32 pma_channel_id)
{
	gb10b_perf_enable_pm_hes_trigger(g, gr_instance_id, pma_channel_id, true);
}

void gb10b_perf_enable_pma_trigger(struct gk20a *g, u32 pma_channel_id)
{
	u32 reg_val = nvgpu_readl(g,
			perf_pmasys_command_slice_trigger_mask_secure0_r(pma_channel_id));

	reg_val |= BIT(perf_pmasys_engine_index_pma_v());
	nvgpu_writel(g, perf_pmasys_command_slice_trigger_mask_secure0_r(pma_channel_id), reg_val);
}

u32 gb10b_perf_get_hwpm_fbp_fbpgs_ltcs_base_addr(void)
{
	return perf_pmmfbp_fbpgs_ltcs_base_v();
}

u32 gb10b_perf_get_hwpm_gpcgs_gpctpca_base_addr(void)
{
	return perf_pmmgpc_gpcgs_gpctpca_base_v();
}

u32 gb10b_perf_get_hwpm_gpcgs_gpctpcb_base_addr(void)
{
	return perf_pmmgpc_gpcgs_gpctpcb_base_v();
}

u32 gb10b_perf_get_hwpm_gpcs_base_addr(void)
{
	return perf_pmmgpc_gpcs_base_v();
}

u32 gb10b_perf_get_hwpm_gpcsrouter_base_addr(void)
{
	return perf_pmmgpc_gpcsrouter_base_v();
}

u32 gb10b_perf_get_hwpm_fbps_base_addr(void)
{
	return perf_pmmfbp_fbps_base_v();
}

u32 gb10b_perf_get_hwpm_fbpsrouter_base_addr(void)
{
	return perf_pmmfbp_fbpsrouter_base_v();
}

void gb10b_perf_reset_hwpm_pmm_register(struct gk20a *g, u32 gr_instance_id)
{
	u32 dgmap_status_reg_count = perf_pmmsysrouter_user_dgmap_status_secure__size_1_v();
	u32 i, count;
	const u32 *perfmon_regs;
	u32 num_gpc = nvgpu_grmgr_get_gr_num_gpcs(g, gr_instance_id);
	u32 num_fbp = nvgpu_grmgr_get_gr_num_fbps(g, gr_instance_id);
	u32 start_gpc = nvgpu_grmgr_get_gr_gpc_logical_id(g, gr_instance_id, 0x0U);
	u32 start_fbp = nvgpu_grmgr_get_fbp_logical_id(g, gr_instance_id, 0x0U);
	u32 *gpc_dg_map_mask;
	u32 gpc_dg_map_mask_size;

	perfmon_regs = g->ops.perf.get_hwpm_sys_perfmon_regs(&count);

	for (i = 0U; i < count; i++) {
		g->ops.perf.set_pmm_register(g, perfmon_regs[i], 0U, 1U,
			g->ops.perf.get_pmmsys_per_chiplet_offset(),
			g->num_sys_perfmon);
	}

	perfmon_regs = g->ops.perf.get_hwpm_fbp_perfmon_regs(&count);

	for (i = 0U; i < count; i++) {
		g->ops.perf.set_pmm_register_for_chiplet_range(g,
			perfmon_regs[i], 0x0U, start_fbp, num_fbp,
			g->ops.perf.get_pmmfbp_per_chiplet_offset(),
			g->num_fbp_perfmon, NULL, 0);
	}

	perfmon_regs = g->ops.perf.get_hwpm_gpc_perfmon_regs(&count);

	/* Allocate buffers to capture dg map mask for GPC, FBP chiplet */
	gpc_dg_map_mask_size = nvgpu_safe_mult_u32(dgmap_status_reg_count, num_gpc);
	gpc_dg_map_mask = nvgpu_kzalloc(g, sizeof(u32) * gpc_dg_map_mask_size);
	nvgpu_assert(gpc_dg_map_mask != NULL);
	gb10b_update_gpc_dg_map_status_mask(g, gr_instance_id, num_gpc, 0x0U, g->num_gpc_perfmon,
					    gpc_dg_map_mask, gpc_dg_map_mask_size);
	for (i = 0U; i < count; i++) {
		g->ops.perf.set_pmm_register_for_chiplet_range(g,
			perfmon_regs[i], 0x0U, start_gpc, num_gpc,
			g->ops.perf.get_pmmgpc_per_chiplet_offset(),
			g->num_gpc_perfmon, gpc_dg_map_mask, gpc_dg_map_mask_size);
	}
	nvgpu_kfree(g, gpc_dg_map_mask);

	if (g->ops.priv_ring.read_pri_fence != NULL) {
		/* Read back to ensure all writes are complete */
		g->ops.priv_ring.read_pri_fence(g);
	}
}

int gb10b_perf_alloc_mem_for_sys_partition_dg_map(struct gk20a *g)
{
	u32 num_instances = g->mig.num_gpu_instances;
	u32 **sys_dg_map;
	u32 i;

	sys_dg_map = nvgpu_kzalloc(g, sizeof(u32) * NVGPU_MIG_MAX_GPU_INSTANCES);
	if (sys_dg_map == NULL) {
		nvgpu_err(g, "mem allocation 1 for dg_map failed");
		return -ENOMEM;
	}

	nvgpu_assert(num_instances < NVGPU_MIG_MAX_GPU_INSTANCES);

	for (i = 0; i < num_instances; i++) {
		sys_dg_map[i] = nvgpu_kzalloc(g, sizeof(u32) *
				perf_pmmsysrouter_user_dgmap_status_secure__size_1_v());
		if (sys_dg_map[i] == NULL) {
			nvgpu_err(g, "mem allocation 2 for dg_map failed for instance: %d", i);
			goto alloc_failed;
		}
	}

	g->sys_partition_dg_map = sys_dg_map;

	return 0;

alloc_failed:
	for (i = 0; i < num_instances; i++) {
		nvgpu_kfree(g, sys_dg_map[i]);
	}
	nvgpu_kfree(g, sys_dg_map);

	return -ENOMEM;
}

/**
 * Programs secure config register for DG's of SYS partition.
 * This API is MIG aware. dg_map_mask contains the
 * bit positions of perfmons that are available based on
 * gpu instance id.
 */
void gb10b_perf_program_sys_pmm_secure_config(struct gk20a *g, u32 offset, u32 val, u32 dg_map_mask)
{
	u32 reg_offset = 0U;
	u32 i;

	for_each_set_bit(i, (const unsigned long *)&dg_map_mask,
			BITS_PER_BYTE * sizeof(dg_map_mask)) {
		reg_offset = offset + (i * perf_pmmsys_perdomain_offset_v());
		nvgpu_writel(g, reg_offset, val);
	}
}
void gb10b_perf_get_tpc_perfmon_range(u32 *perfmon_start_idx,
					u32 *perfmon_end_idx)
{
	*perfmon_start_idx =  perf_gpc_streaming_dg_id_gpctpca0_v();
	*perfmon_end_idx = perf_gpc_streaming_dg_id_gpctpcd3_v();
}

u32 gb10b_perf_get_max_num_gpc_perfmons(void)
{
	return perf_pmmgpc_control__size_1_v();
}

u32 gb10b_perf_get_gpc_perfmon_stride(void)
{
	return perf_pmmgpc_perdomain_offset_v();
}

/*
 * HS Credits are caluclated based on the bandwidth of the bus connecting the pmmrouters and pma.
 * "NUM_OF_CREDITS >= ROUND_TRIP_TIME * INTERFACE_SZ / SZ_OF_RECORD". And total credits thats can
 * be used across all pmmrouters are 256, which is as per hw limitation.
 */
u32 gb10b_perf_get_hs_credit_per_gpc_chiplet(struct gk20a *g)
{
	(void)g;

	/*
	 * For all chiplets combined credits should be >= 320 * 16 / 32 = 160.
	 * With some increase it is fixed to 168 and divided by num of chiplets.
	 */
	return 56U;
}

u32 gb10b_perf_get_hs_credit_per_fbp_chiplet(struct gk20a *g)
{
	(void)g;

	/*
	 * Per FBP router credits should be >= 320 * 0.5 / 12 = 4.
	 * With more credit allocated it is fixed at 8U.
	 * Each fBP is connected to the pma through dedicated CXBAR
	 */
	return 8U;
}

/*
 * Sys pmmrouters are directly connected to pma.
 * so credits >= (16 * 32 / 32 = 16). With more credit allocation from max limit we fix
 * 20 credits for sys with hwpm perfmons and 36 credits for sys with hes perfmons.
 */
u32 gb10b_perf_get_hs_credit_per_sys_pipe_for_profiling(struct gk20a *g)
{
	(void)g;

	return 10U;
}

u32 gb10b_perf_get_hs_credit_per_sys_pipe_for_hes(struct gk20a *g)
{
	(void)g;

	return 18U;
}

/*
 * Program the HS credits per chiplet index of a chiplet type.
 */
void gb10b_perf_set_hs_credit_per_chiplet(struct gk20a *g, u32 gr_instance_id,
			u32 pma_channel_id, u32 chiplet_type,
			u32 chiplet_local_index, u32 num_of_credits)
{
	u32 cblock_idx = PMA_CBLOCK_IDX(pma_channel_id);
	u32 channel_idx = PMA_CBLOCK_CH_IDX(pma_channel_id);
	u32 logical_id, pmmrouter_stride, offset = 0U;
	u32 val;

	if (chiplet_type == NVGPU_PROFILER_CHANNEL_CHIPLET_TYPE_GPC) {
		pmmrouter_stride = perf_pmmgpcrouter_extent_v() - perf_pmmgpcrouter_base_v() + 1U;
		nvgpu_assert(chiplet_local_index < nvgpu_grmgr_get_gr_num_gpcs(g, gr_instance_id));

		logical_id = nvgpu_grmgr_get_gr_gpc_logical_id(g, gr_instance_id, chiplet_local_index);
		offset = nvgpu_safe_add_u32(perf_pmmgpc_gpc0_config_r(cblock_idx, channel_idx),
					nvgpu_safe_mult_u32(pmmrouter_stride, logical_id));
	} else if (chiplet_type == NVGPU_PROFILER_CHANNEL_CHIPLET_TYPE_FBP) {
		pmmrouter_stride = perf_pmmfbprouter_extent_v() - perf_pmmfbprouter_base_v() + 1U;
		nvgpu_assert(chiplet_local_index < nvgpu_grmgr_get_gr_num_fbps(g, gr_instance_id));

		logical_id = nvgpu_grmgr_get_fbp_logical_id(g, gr_instance_id, chiplet_local_index);
		offset = nvgpu_safe_add_u32(perf_pmmfbp_fbp0_config_r(cblock_idx, channel_idx),
					nvgpu_safe_mult_u32(pmmrouter_stride, logical_id));
	} else if (chiplet_type == NVGPU_PROFILER_CHANNEL_CHIPLET_TYPE_SYS) {
		offset = perf_pmmsys_sys0_config_r(cblock_idx, channel_idx);
	} else {
		/* This invalid state should have been pruned at caller itself */
		nvgpu_assert(1);
		return;
	}

	val = nvgpu_readl(g, offset);
	val = set_field(val, perf_pmmgpc_gpc0_config_value_m(),
			perf_pmmgpc_gpc0_config_value_f(num_of_credits));
	nvgpu_writel(g, offset, val);

	nvgpu_log(g, gpu_dbg_prof, "gr_instance_id: %u, offset: 0X%X, val: 0X%X",
					gr_instance_id,	offset, val);
}

void gb10b_perf_set_pma_stream_gfid(struct gk20a *g, u32 pma_channel_id, u32 gfid)
{
	u32 cblock_idx = PMA_CBLOCK_IDX(pma_channel_id);

	nvgpu_assert(cblock_idx < g->ops.perf.get_pma_cblock_instance_count());
	nvgpu_writel(g, perf_pmasys_cblock_bpc_gfid_r(cblock_idx), gfid);
}

bool gb10b_perf_is_perfmon_simulated(void)
{
	return false;
}
